{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8204,"sourceType":"datasetVersion","datasetId":4458}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INTRODUCTION TO THE NOTEBOOK\n\n## Aim\n\nIn this notebook, we will explore the principal characteristics of the linear regression models. This document is designed to emphasize technical details over the typical step-by-step procedure for implementing a regression model. Consequently, the organization of sections does not follow the traditional sequence. A complementary document will be provided, featuring a practical example to cover it thoroughly.\n\nPlease feel free to suggest any corrections, modifications, or improvements. **Your feedback is greatly appreciated**.\n\n## Programming Language\n\nThe code in the following sections is developed using Python (v. 3.10.13). The versions of the packages used are:\n\n* **Pandas**: '2.2.2' \n* **scikit-learn**: '1.2.2' \n* **Matplotlib**: '3.7.5'\n* **Numpy**: '1.26.4'\n* **Statsmodels**: '0.14.1'","metadata":{}},{"cell_type":"code","source":"import pandas as pd                                  # Read csv\nfrom sklearn.linear_model  import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures # interaction effect, no linear models\nfrom sklearn.linear_model  import Ridge, RidgeCV, Lasso, LassoCV # regularizationy, best value of alpha Ridge, reduce features, best value of alpha Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.dummy import DummyRegressor\nimport matplotlib\nfrom sklearn.preprocessing import StandardScaler # standardization\nimport matplotlib.pyplot as plt                  # plot\nimport numpy as np\nimport statsmodels.api as sm\n\n\n# Check Python and packages version.\nimport sys\nfrom sklearn import __version__ as sklearn_version # Only needed for sklearn\n\nversions = {\n    \"Python\": sys.version.split(\" \")[0],  # Simplifying to just the version number\n    \"Pandas\": pd.__version__,\n    \"scikit-learn\": sklearn_version,\n    \"Matplotlib\": matplotlib.__version__,\n    \"Numpy\": np.__version__,\n    \"Statsmodels\": sm.__version__\n}\n\n# print(versions)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T16:13:25.556031Z","iopub.execute_input":"2024-05-09T16:13:25.556661Z","iopub.status.idle":"2024-05-09T16:13:25.567145Z","shell.execute_reply.started":"2024-05-09T16:13:25.556617Z","shell.execute_reply":"2024-05-09T16:13:25.565878Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Database\n\nThroughout this notebook, we will use the \"Red Wine Quality\" database. You can find a complete description of this dataset by following this link: [Red Wine Quality Dataset Description](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009).\n\n* All the variable in this database are quantitative.\n* The target variable is \"quality\", which ranges from 0 to 10.\n\nNow, we are going to upload the data and separate target from features. There are two important things that this notebook ommit:\n\n* Split data in training and test. \n* Explore and clean the data.\n* Check the assumptions of the model.\n\nYou can see it in practise in the complementary example.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Read csv and see the chaacteristics\ndf = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nprint(\"Number of rows:\", len(df))\nprint(df.head(5))\n\n# Separate target and features:\n\ndf_target   = df[['quality']]      # Use doble [[ ]] to avoid transform in serie (because is only one column)\ndf_features = df.drop('quality', axis=1)\n\nprint(\"\\n\")\nprint(\"The compleate df has\", len(df),          \"cases and\", df.shape[1],          \"columns\")\nprint(\"df_target has\",        len(df_target),   \"cases and\", df_target.shape[1],   \"column\")\nprint(\"df_features has\",      len(df_features), \"cases and\", df_features.shape[1], \"columns\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T16:29:35.987211Z","iopub.execute_input":"2024-05-09T16:29:35.987672Z","iopub.status.idle":"2024-05-09T16:29:36.015865Z","shell.execute_reply.started":"2024-05-09T16:29:35.987636Z","shell.execute_reply":"2024-05-09T16:29:36.014890Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of rows: 1599\n   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  quality  \n0      9.4        5  \n1      9.8        5  \n2      9.8        5  \n3      9.8        6  \n4      9.4        5  \n\n\nThe compleate df has 1599 cases and 12 columns\ndf_target has 1599 cases and 1 column\ndf_features has 1599 cases and 11 columns\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Summary of the data**\n\nWe have 1,599 entries (cases,rows), one target variable (\"quality\") and 11 features.","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction to Linear Regression\n\nLinear regression is a machine learning algorithm classified under \"supervised learning\" techniques.\n\nThe **aim** of linear regression algorithm is to predict one variable (VD, also known as *target* or *y*) using one or more features (VI, *predictors* or *$x_i$*).\n\nThere are two type of models depending on the number of features involve:\n\n* **Simple Linear Regression**. The model only one feature to predict the target:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\epsilon\n$$\n\n* **Multiple Linear Regression**. This model use two or more features to predict the target.\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 +  \\hat{\\beta}_2 x_2 + ... + \\hat{\\beta}_n x_n + \\epsilon\n$$\n\nWhere:\n\n* $ \\hat{y} $: Is the target we predict.\n* $ \\hat{\\beta}_0 $: Is the *intercept* or *bias*.\n* $ \\hat{\\beta}_1, \\hat{\\beta}_2, ... $: Are the *coefficients* (*weights*, *effect*) associated with each feature. Is the effect of one feature on the target.\n* $ \\epsilon $: Represents the error term.\n\n\n**Note**: When discussing estimations (like parameter estimations or model residuals) or predictions, we use the hat notation, as in $\\hat{y}$.","metadata":{}},{"cell_type":"markdown","source":"## A. Model 1: Simple Linear Regression Model\n\nThis model uses only one feature to predict the \"quality\" of the wine, which is our target variable stored in *df_target*. \n\n\nThe predictor we will use is the \"alcohol\" level, currently housed within *df_features* along with other features. To simplify our analysis, we will first extract this specific feature into a new object dedicated solely to it.","metadata":{}},{"cell_type":"code","source":"# Select only the feature alcohol\ndf_feature_one = df_features[['alcohol']]\n\n# Print results\nprint(\"Target quality \\n\", df_target.sort_index().head(5), \"\\n\")\nprint(\"Feature alcohol \\n\", df_feature_one.sort_index().head(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T16:55:05.320937Z","iopub.execute_input":"2024-05-09T16:55:05.321467Z","iopub.status.idle":"2024-05-09T16:55:05.335102Z","shell.execute_reply.started":"2024-05-09T16:55:05.321423Z","shell.execute_reply":"2024-05-09T16:55:05.333891Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Target quality \n    quality\n0        5\n1        5\n2        5\n3        6\n4        5 \n\nFeature alcohol \n    alcohol\n0      9.4\n1      9.8\n2      9.8\n3      9.8\n4      9.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, we'll construct the model to establish the relationship between \"quality\" and \"alcohol\" content. Here's the step-by-step process:\n\n* **Create the model**. In this case, Linear Regression model.\n* **Fit the model**. Train the model with our data. During this process, the model will adjust its parameters to best fit the relationship between quality and alcohol content. As a result, we'll obtain the coefficients and the intercept.\n* **Display Coefficients and Intercept**: After fitting the model, we'll showcase the numerical values for the intercept and coefficients. These values provide insights into the baseline quality (intercept) and the magnitude of influence (coefficients) that alcohol content has on the quality of the product.","metadata":{}},{"cell_type":"code","source":"# Create SLRM and fit it\nregression1 = LinearRegression()\n\n# Fit the model\nmodel1 = regression1.fit(df_feature_one.sort_index(),   # Features first then target\n                         df_target.sort_index()) \n\n# Show the results (round to three decimals):\nprint(\"Intercept:\",     model1.intercept_[0].round(3))\nprint(\"Coefficients :\", model1.coef_[0][0].round(3))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:24:19.535933Z","iopub.execute_input":"2024-05-09T17:24:19.536378Z","iopub.status.idle":"2024-05-09T17:24:19.551457Z","shell.execute_reply.started":"2024-05-09T17:24:19.536343Z","shell.execute_reply":"2024-05-09T17:24:19.549805Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Intercept: 1.875\nCoefficients : 0.361\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Whith this information we can build the equation of the model:\n\n$$\n\\hat{y} = 1.830 + 0.364 · x_1 + \\epsilon\n$$\n\nThe error $ \\epsilon $ is the difference between the real value of $ y $ and the predicted value $ \\hat{y} $. We only can know that value when we have the real result but not when we predict.\n\nHowever, we can replace the feature value in the equation. For our first case this value is:","metadata":{}},{"cell_type":"code","source":"print(\"Value of alcohol feature for our first case :\", df_train_feature_alcohol.iloc[0:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, the equation for the first case is:\n\n$$\n\\hat{y} = 1.830 + 0.364 · 10 \n$$\n\nSo the quality predicted value of our firsy case is:\n\n$$\n\\hat{y} = 1.830 + 0.364 · 10 \\approx 5.473\n$$\n\nWe can do this using our script:","metadata":{}},{"cell_type":"code","source":"model1_y = pd.DataFrame(model1.predict(df_train_feature_alcohol.sort_index()), \n                      index=df_train_feature_alcohol.index, \n                      columns=['Simple Model'])\n\nprint(\"Quality predicted value for the frst case:\", model1_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are going to plot our model. This graphics shows the relation between real values of alcohol (x-exe) and real quality (y-exes). The red line represents the fitted linear regression model. More close the dots to the line, better predictions we made.","metadata":{}},{"cell_type":"code","source":"# Plotting the actual data points\nplt.figure(figsize=(10, 6))\nplt.scatter(df_train_feature_alcohol, df_train_target, color='blue', label='Actual data')\n\n# Plotting the regression line\nplt.plot(df_train_feature_alcohol, model1.predict(df_train_feature_alcohol), color='red', linewidth=2, label='Fitted line')\n\n# Limits of the axes\nplt.xlim(4, 16)\nplt.ylim(0, 10)\n\n# Adding labels and title\nplt.ylabel('Quality (real value)')\nplt.xlabel('Alcohol (feature)')\nplt.title('Simple Linear Regression Fit')\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is what we can conclude only with the plot:\n\n* Dots follow the vertical. This indicated that the alcohol is not the unique variable that could explain the quality of the wine.\n* The prediction line increase. We could translate this as \"more alcohol, more quality of the wine\".\n* Dots are not very close to the line what means that probably there are other factor that are affecting in the quality.","metadata":{}},{"cell_type":"markdown","source":"### B. Compute our first multiple linear regression model\n\nIn this case, we are going to include all the features that we have to predict the quality (target). The steps are as in the simple model.","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression2 = LinearRegression()\n\n# Fit the model\nmodel2 = regression2.fit(df_train_features.sort_index(), df_train_target.sort_index()) # Features first then target\n\n# Show the equation values:\nprint(\"Intercept :\", model2.intercept_ )\nprint(\"Coefficients :\", model2.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whith this information we can build the equation of the model:\n\n$$\n\\hat{y} = 16.635 + 0.020 · x_1 - 1.026 · x_2 - 0.151 · x_3 + 0.012 · x_4 - 1.490 · x_5 + 0.005 · x_6 - 0.003 · x_7 - 12.367 · x_8 - 0.534 · x_9 + 0.887 · x_{10} + 0.298 · x_{11}\n$$\n\nWe can see that we have negative and positive values in the coeficients. We interpretate as follow:\n\n* **Positive**: A high value in that feature, increase the value of the taarget.\n* **Negative**: A high value in that feature, decrease the value of the taarget.\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features.iloc[0:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the first case the final equation looks like:\n\n$$\n\\hat{y} = 16.634 + 0.020 \\cdot 12.8 - 1.026 \\cdot 0.615 - 0.151 \\cdot 0.66 + 0.012 \\cdot 5.8 - 1.490 \\cdot 0.083 + 0.005 \\cdot 7.0 - 0.003 \\cdot 42.0 - 12.367 \\cdot 1.0022 - 0.534 \\cdot 3.07 + 0.887 \\cdot 0.73 + 0.298 \\cdot 10.0\n$$\n\nAnd the quality predicted value :\n\n$$\n\\hat{y} \\approx 5.608\n$$\n\nWe can do this using our script:","metadata":{}},{"cell_type":"code","source":"model2_y = pd.DataFrame(model2.predict(df_train_features), \n                      index=df_train_features.index, \n                      columns=['Multiple Model'])\n\nprint(\"Quality predicted value for the frst case:\", model2_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Represent now in a plot","metadata":{}},{"cell_type":"code","source":"# Plotting the actual data points\nplt.figure(figsize=(10, 6))\nplt.scatter(model2.predict(df_train_features.sort_index()), df_train_target.sort_index(), color='blue', label='Predicted vs Actual data')\n\n# Plotting the regression line\nplt.plot(df_train_target.sort_index(), df_train_target.sort_index(), color='red', linewidth=2, label='Fitted line')\n\n# Limits of the axes\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n\n# Adding labels and title\nplt.xlabel('Quality (real value)')\nplt.ylabel('Predicted quality')\nplt.title('Multiple Linear Regression Fit')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Features modifications \n\n### A. Interacction effect\n\nSome times, features could not show an effect in the target by it selfs, but the combinations of them does. We can include this type of effect including an interaction term in the model. \n\nIncluding interaction terms in the model allows for the possibility that the relationship between the predictors and the target variable is not additive ($ \\hat{\\beta}_1 · x_1 +  \\hat{\\beta}_2 · x_2 $) but varies depending on the levels of other predictors ($ \\hat{\\beta}_3 · x_1 · x_2 $).\n\nImagine we have an original model with two features: x1 and x2. As we see the introduction, the model will take this form:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\epsilon\n$$\n\nIf we suspect some interaction between this two features, we can add the interaction effect as follow:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1 x_2 + \\epsilon\n$$\n\nSome relevant things:\n* We can include as many interaction effects as we want, if there are interesting. However, think in the relevance for include them because we take the risk of overfitting in our model.\n* An interaction effect could include more than two features. For example ($ \\hat{\\beta}_3 · x_1 · x_2 · x_3 $). If you decide this is interesting in your case, take care about the overfitting and the dificulty of interpreate the interaction.\n* It is not necessary include interaction for all the features involve in the model. If we have a model with three features, and we thing only the interaction between $ x_1 $ and $ x_2 $ is ineresting, then we can create a model like this: $ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + \\hat{\\beta}_4 x_1 x_2 + \\epsilon $\n\nThe domain knowledge and a visualization that shows the effect in the target using two features could be useful to decide when include an interaction.\n\nWe will se how to do this in python.\n","metadata":{}},{"cell_type":"markdown","source":"### Multi-linear regression model all features plus an interaction between two of them\n\nAs before, alochol is our target variable. As predictors we include all the feautres plus the interaction between free sulfur dioxide and alochol.\n\nFirst, we create the interaction:\n\n","metadata":{}},{"cell_type":"code","source":"interaction = PolynomialFeatures(\n    degree=2,                    # How many features combinations we want to create\n    include_bias=False,          # Exclude the bias term \n    interaction_only=True        # Only include interaction effects (not x^2, x^3, etc.)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have to transform and fit the interaction between free sulfur dioxide and alcohol:","metadata":{}},{"cell_type":"code","source":"features_interaction = interaction.fit_transform(df_train_features[['free sulfur dioxide', 'alcohol']])\n\n# This is not necessary but is good to see the results. Transform the array into a dataframe:\nfeatures_interaction = pd.DataFrame(features_interaction,\n                                      columns = interaction.get_feature_names_out(['free sulfur dioxide', 'alcohol']),\n                                      index=df_train_features.index  # This line ensures the index is carried over. Example, index 10 doesn´t should appear\n                                   )\nprint(features_interaction.sort_index().head(1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, the interaction include the simple features (because degree = 2 includes degree = 1 that are the simple effects) and the interaction. We are only interested in the interaction column (free sulfur dioxide alcohol) so we extract only that column and we add it to our original features df: ","metadata":{}},{"cell_type":"code","source":"interaction_column = features_interaction[['free sulfur dioxide alcohol']].sort_index()\nprint(interaction_column.sort_index().head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We add the interaction column to our original feature df. We are going to create a new df to show that is a modified version.","metadata":{}},{"cell_type":"code","source":"df_train_features_modified = df_train_features.join(interaction_column, how='outer')\npd.set_option('display.max_rows', 1600) \nprint(df_train_features_modified.sort_index().head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have a new df with all the original features plus one with the interaction (the last one).\n\n**!!! Important**. We have done this only for train data. In a normal workflow, this step must be done **before** split the df in test and training. In the final example we are going to see a compleate process.\n\nWith our final df, features_interaction_modified, we can fit the model and obtain the intercept and coefficients:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression3 = LinearRegression()\n\n# Fit the model\nmodel3 = regression3.fit(df_train_features_modified, df_train_target.sort_index()) # Features first then target\n\n# Show the equation values:\nprint(\"Intercept :\", model3.intercept_ )\n\n# New way to obtain the coefficients with names and round them to three decimals:\nfeature_names = df_train_features_modified.columns\ncoefficient_dict = dict(zip(feature_names, model3.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = -19.912 -0.013 · x_1 - 0.239 · x_2 + 0.199 · x_3 - 0.022 · x_4 + 0.454 · x_5 + 0.007 · x_6 + 0.001 · x_7 + 24.904 · x_8 + 0.136 · x_9 - 0.257 · x_{10} + 0.054 · x_{11} - 0.0005 · x_{6} · x_{11}\n$$\n\nAs we see, the interaction coefficient is negative. It means that when we have higher levels of alcohol, the effect of free sulfur dioxide in the quality is less positive or negative. However, as we see, the size of the coefficients is ver low.\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"# Notice that we use df_train_features_modified\nprint(\"Value of all features for our first case :\", df_train_features_modified.iloc[0:1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We replace the variable information:","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T13:45:11.367516Z","iopub.execute_input":"2024-04-30T13:45:11.368509Z","iopub.status.idle":"2024-04-30T13:45:11.379680Z","shell.execute_reply.started":"2024-04-30T13:45:11.368447Z","shell.execute_reply":"2024-04-30T13:45:11.377633Z"}}},{"cell_type":"code","source":"model3_y = pd.DataFrame(model3.predict(df_train_features_modified), \n                      index=df_train_features_modified.index, \n                      columns=['Interaction Model'])\n\nprint(\"Quality predicted value for the frst case:\", model3_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An error occurred while committing kernel: ConcurrencyViolation Sequence number must match Draft record: KernelId=55703157, ExpectedSequence=611, ActualSequence=608, AuthorUserId=4700056\n### B. No linear relations\n\nIn linear model we expect the relation between target and feature will be constant. When it does't happen the relation is no lineal. For example, imagine a student that is preparing an exam. In general, more hours of study means a better score. However, this relation is not linear. Study one hour doesn't mean obtain a one score in the test, or study seven hour doesn't mean score as seven. The relation is exponencial between the target (score) and the feature (number of hours of study).\n\nA model that include a no lineal effect in a feature take this aspect:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 +  \\hat{\\beta}_2 x_1^2 + ... + \\hat{\\beta}_n x_n + \\epsilon\n$$\n\nWhere:\n\n* $ x_1 $: Is the feature with a lineal efect.\n* $ x_1^2 $: Is the same feature with a no-linear association. $ ^2 $ are the grades of the polinomio.\n\nThe procedure is similar to the interaction effect. We are going to create the compleate model including a no linear regression for sulphite feature.\n\nWe start creating a new feature $ sulphite^2 $ that is the same as $ suphite · sulphite $ (a interaction between the same feature).","metadata":{}},{"cell_type":"code","source":"polynomial = PolynomialFeatures(\n    degree=2,                    # We need a interaction with two degrees.\n    include_bias=False,          # Exclude the bias term \n    interaction_only=False       # NEW. Include polynomia effects. In our case only x^2 because we are interesting in 2 degrees.\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the model:","metadata":{}},{"cell_type":"code","source":"features_polynomial = polynomial.fit_transform(df_train_features[['sulphates']].sort_index())\n\n# This is not necessary but is good to see the results. Transform the array into a dataframe:\nfeatures_polynomial = pd.DataFrame(features_polynomial,\n                                   columns = polynomial.get_feature_names_out(),\n                                   index=df_train_features.index  # This line ensures the index is carried over. Example, index 10 doesn´t should appear\n                                   )\nprint(features_polynomial.sort_index().head(20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need only the $ sulphates^2 $ column. First, we isolete that column and then we add it to the original feature df (we create a new dataframe for this new data)","metadata":{}},{"cell_type":"code","source":"# Isolate the polynomial feature\npolynomial_column = features_polynomial[['sulphates^2']].sort_index()\n\n# Add to the other features (and create a new features df)\ndf_train_features_polynomial = df_train_features.join(polynomial_column, how='outer')\nprint(df_train_features_polynomial.sort_index().head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**!!! Important**. We have done this only for train data. In a normal workflow, this step must be done **before** split the df in test and training. In the final example we are going to see a compleate process.\n\nWith our final df, features_interaction_modified, we can fit the model and obtain the intercept and coefficients:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression4 = LinearRegression()\n\n# Fit the model\nmodel4 = regression4.fit(df_train_features_polynomial.sort_index(), df_train_target.sort_index()) # Features first then target\n\n# Show the equation values:\nprint(\"Intercept :\", model4.intercept_ )\n\n# New way to obtain the coefficients with names and round them to three decimals:\nfeature_names = df_train_features_polynomial.columns\ncoefficient_dict = dict(zip(feature_names, model4.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = -19.002 -0.014 · x_1 - 0.229 · x_2 + 0.199 · x_3 - 0.021 · x_4 + 0.467 · x_5 + 0.002 · x_6 + 0.001 · x_7 + 24.062 · x_8 + 0.114 · x_9 + 0.011 · x_{10} + 0.045 · x_{11} - 0.158 · x_{10}^2\n$$\n\nAs we see, the polynomial coefficient is negative. It means that when we have higher levels of sulphates, the level of quality (target) decrees. As is a polynomial feature, it means that the decrising on quality is faster.\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"model4_y = pd.DataFrame(model4.predict(df_train_features_polynomial.sort_index()), \n                      index=df_train_features_polynomial.index, \n                      columns=['Polinomyal Model'])\n\nprint(\"Quality predicted value for the frst case:\", model4_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a plot comparing the predicted value (x-axe) vs the real on (y-axe). We round our predicted value to 0 decimal because the original scale of quality (target) is an integer with no decimals. As we can see, our model predicts for all wines qualities between 5 and 6 while the original values of qualities range between 3 and 8.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.scatter(np.round(model4.predict(df_train_features_polynomial)), df_train_target.sort_index(), c=df_train_target, cmap='bwr', edgecolors='k', s=100)\nplt.xlim(0, 10)\nplt.ylim(0, 10)\nplt.xlabel('Features')\nplt.ylabel('Target')\nplt.title('Logistic Regression with Polynomial Feature')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Optimization techniques\n\nScikit-learn give a lot of options about what estimator use. The image bellow shows the huge variety this package includes, not only for regression, if not different techniques. You can see more information in https://scikit-learn.org/stable/_static/ml_map\n\n<div style=\"text-align:center\">\n    <img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"Scikit-Learn estimators\" width=\"700\" height=\"500\">\n</div>\n\nThe **optimization algorithms** are techniques used to find the best solution to a problem, usually trying to fin the minimun or the maximun in a function. Some key concepts:\n\n* **Objetive function**: Function we want to optimize.\n* **Minimun and Maximun**: We look for the minimun when we try to obtain the parameters that minimze the error. We look for the maximun when try to found the features that maximize the gaprofitsins.\n* **Stopping Conditions**: Criterial that sais when stop to search for the optimal solution. For example, we fix the number of iterations or when the improvements in the objetive function is bellow a specif threshold.\n* **Optimal point**: Input value that produces the minimum or maximun value of the objetive function.\n* **Gradient**: Vector that inicates the direction and magnitude of the steepest change in a function.\n\n**Residual Sum of Squares (RSS)**: Metric to compute the differences between real values and estimate values. It follows this formula:\n\n$$\nRSS = \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2\n$$\n\nWhere:\n\n* i: is the specific row.\n* n: is the total sample.\n* $y_i$ is the real value of the target.\n* $\\hat{y}_i$: is the estimated value of the target.\n\nRegularization model includes some penalization to this RSS.","metadata":{}},{"cell_type":"markdown","source":"### A. Ordinary Least Square (OLS)\n\n* The aim is minimize the square sum of the errors (differences between predictions and real values). \n* Usefull when the errors follows a normal distribution and there are no outliers.\n","metadata":{}},{"cell_type":"markdown","source":"### B. Grdient Descent\n\n* Iterative process to find the optime parameters.\n* Use all the sample.\n* Parameter are update in the oposite direction of the gradient of the loss function.\n* Search the local or global minimun.\n* Slower than other model when the linear regression problem is simple but is more flexible so it is useful with complex problems.","metadata":{}},{"cell_type":"markdown","source":"### C. SGD Regressor (Stochastic Gradient Descent)\n\n* Variant od Gradient Descent.\n* Search for the minimun to update the model parameters.\n* Use when you have large samples.\n* Use a random sample of the data in each iteration. This is the reason to use with large datasets (large training samples).\n","metadata":{}},{"cell_type":"markdown","source":"### D. Regularization\n\nDifferences between Lasso and Ridge Regression:\n\n| | Ridge Regression | Lasso Regression |\n|:--:| :--:|:--:|\n|Penalty term| Proportional to the square of the square of the magnitude of the coefficients: $ \\Theta^2 $ | Proportional to the absolute value of the coefficients $ |\\Theta| $. It could drive some coefficients to 0|\n|Aim| Reduce the overfit and reduce the complexity of the model| Reduce the complexity and automatically select some features (driven their coefficients to 0, in other words, deleting it)|\n|Predictions| Better| More interpretable|\n\n**$\\alpha$**: Hiperparameter that controls how much is going to be penalize. Higher the value, more simple models. To select the best value, a tuning process must be done. **WARNING** in Lasso, if we pick a large value we can deleate a lot of features because their coefficients will go to 0.\n\n#### A. Ridge Regression\n\nWhen use it:\n* Big correlation between features\n* Number of features > number of observations (sobredimensional models)\n* When we want to reduce the overfiting but not the multicollinearity.\n\nThis model penalizes as follow:\n\n$$\nRSS + \\alpha ·  \\sum_{j=1}^{p} (\\hat{\\beta} _j )^2\n$$\n\nWhere:\n\n* j: is a specific feature.\n* p: is total number of features.\n* $\\beta$ is the coeficient of the feature.\n* $\\alpha$: is the hiperparameter.\n\n#### B. Lasso Regression\n\nThis model penalizes as follow:\n\n$$\n\\frac{1}{2n}· RSS + \\alpha ·  \\sum_{j=1}^{p} |\\hat{\\beta}_j|\n$$\n\nWhere:\n\n* j: is a specific feature.\n* p: is total number of features.\n* $\\beta$ is the coeficient of the feature.\n* $\\alpha$: is the hiperparameter.\n* n: Number of obervations.\n\n#### C. Elastic Net\n\nCombine Ridge and Lasso.\n","metadata":{}},{"cell_type":"markdown","source":"#### Ridge Regression\n\nCreate the Ridge Regression Model and select different values for the hiperparameter $\\alpha$","metadata":{}},{"cell_type":"code","source":"# Create the model with different values for alpha\nridge_regression = RidgeCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adjust the model:","metadata":{}},{"cell_type":"code","source":"ridge_model = ridge_regression.fit(df_train_features.sort_index(), df_train_target.sort_index())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the coefficients and the best value of $\\alpha$:","metadata":{}},{"cell_type":"code","source":"print(\"Coefficients :\", ridge_model.alpha_)\nprint(\"Intercept :\", ridge_model.intercept_ )\n\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, ridge_model.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 4.573 + 0.009 · x_1 -1.037 · x_2 - 0.153 · x_3 + 0.0011 · x_4 + - 1.427 · x_5 + 0.006 · x_6 - 0.003 · x_7 - 0.093 · x_8 - 0.586 · x_9 + 0.861 · x_{10} + 0.309 · x_{11} \n$$\n\n\nAs we see, the polynomial coefficient is negative. It means that when we have higher levels of sulphates, the level of quality (target) decrees. As is a polynomial feature, it means that the decrising on quality is faster.\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features.sort_index().iloc[0:1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the equation:\n\n$$\n\\hat{y} = 4.573 + 0.009 · 7.8 - 1.037 · 0.88 - 0.153 · 0 + 0.001 · 2.6 - 1.427 · 0.098 + 0.006 · 25 - 0.003 · 67 - 0.093 · 0.997 - 0.586 · 3.2 + 0.861 · 0.68 + 0.309 · 9.8 \\approx 5.188\n$$\n\nIf we compute in python:","metadata":{}},{"cell_type":"code","source":"ridge_model_y = pd.DataFrame(ridge_model.predict(df_train_features), \n                      index=df_train_features.index, \n                      columns=['Ridge Model'])\n\nprint(\"Quality predicted value for the frst case:\", ridge_model_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lasso Regression\n\nCreate the Lasso Regression Model and select different values for the hiperparameter $\\alpha$","metadata":{}},{"cell_type":"code","source":"lasso_regression = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adjust the model:","metadata":{}},{"cell_type":"code","source":"lasso_model = lasso_regression.fit(df_train_features, df_train_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the coefficients and the best value of $\\alpha$:","metadata":{}},{"cell_type":"code","source":"print(\"Coefficients :\", lasso_model.alpha_)\nprint(\"Intercept :\", lasso_model.intercept_ )\n\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, lasso_model.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 3.981 + 0.012 · x_1 -1.012 · x_2 - 0.074 · x_3 + 0.004 · x_4 + - 0.803 · x_5 + 0.006 · x_6 - 0.003 · x_7 - 0 · x_8 - 0.457 · x_9 + 0.768 · x_{10} + 0.311 · x_{11} \n$$\n\n\nAs we see, tthe coefficient for *density* drops to 0\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features.sort_index().iloc[0:1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the equation:\n\n$$\n\n\\hat{y} = 3.981 + 0.012 · 7.8 -1.012 · 0.88 - 0.074 · 0 + 0.004 · 2.6 + - 0.803 · 0.098 + 0.006 · 25 - 0.003 · 67 - 0 · 0.997 - 0.457 · 3.2 + 0.768 · 0.68 + 0.311 · 9.8 \\approx 5.172\n\n$$\n\nIf we compute in python:","metadata":{}},{"cell_type":"code","source":"lasso_model_y = pd.DataFrame(lasso_model.predict(df_train_features), \n                      index=df_train_features.index, \n                      columns=['Lasso Model'])\n\nprint(\"Quality predicted value for the first case:\", lasso_model_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Quantiy the fit\n\n### A. Coefficient of determination $R^2$\n\nDefinition: \"Proportion of target varianze that could be predict using the targeys\".\n\nThis coefficient is used to compare the efficiency of different models and determine which of them fix better the observate data.\n\nThe formula is as follow:\n\n$$\nR^2 = 1 - \\frac{SS_{res}}{SS_{total}}\n$$\n\n* Residual Sum Square ($SS_{res}$): \n\n$$\nSS_{res} = \\sum(y_i - \\hat{y_i})^2\n$$\n\n* Total Sum Square ($SS_{total}$): \n\n$$\nSS_{total} = \\sum(y_i - \\bar{y_i})^2\n$$\n\nThe nomenclature used in this is formulas:\n\n* $y_i$: Real values of the target.\n* $\\hat{y_i}$: Target values predicted by the model.\n* $\\bar{y_i}$: Mean of the real values ($y_i$).\n\nInterpretation:\n\n* $R^2$ = 1. The model is perfect. The features explain all the variability of the target. The residues are minimus.\n* $R^2$ = 0. The model doesn´t explain the variability in the data. Is not more useful than use the mean of the data as a predictor of each observation.\n* $R^2$ < 0. The model made worse than using simply the mean.\n\nCharacteristics:\n\n* Although it is odd, the model could be worse than use only the mean.\n\n* When the prediction residuals have zero mean, the score is identical to the Explained Variance score.\n\nWe are going to explore the models that we generate in previous steps:","metadata":{}},{"cell_type":"code","source":"# Sort by index\ndf_train_target = df_train_target.sort_index()\nmodel1_y = model1_y.sort_index()\nmodel2_y = model2_y.sort_index()\nmodel3_y = model3_y.sort_index()\nmodel4_y = model4_y.sort_index()\nridge_model_y = ridge_model_y.sort_index()\nlasso_model_y = lasso_model_y.sort_index()\n\n# Ensure all preidctions and real observations has the same index:\nassert df_train_target.index.equals(model1_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model2_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model3_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model4_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(ridge_model_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(lasso_model_y.index), \"Indices do not match\"\nprint(\"Indices match successfully!\")\n\npredictions = pd.DataFrame({\n    'Actual Target': np.squeeze(df_train_target), # Real values\n    'Simple Model': np.squeeze(model1_y),\n    'Multiple Model': np.squeeze(model2_y),\n    'Interaction Model': np.squeeze(model3_y),\n    'Polynomial Model': np.squeeze(model4_y),\n    'Ridge Model': np.squeeze(ridge_model_y),\n    'Lasso Model': np.squeeze(lasso_model_y),\n})\n\nprint(predictions.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to calculate the $R^2$ coefficient:","metadata":{}},{"cell_type":"code","source":"r2_scores = {}\nr2_scores['Simple Model'] = r2_score(predictions['Actual Target'], predictions['Simple Model'])\nr2_scores['Multiple Model'] = r2_score(predictions['Actual Target'], predictions['Multiple Model'])\nr2_scores['Interaction Model'] = r2_score(predictions['Actual Target'], predictions['Interaction Model'])\nr2_scores['Polynomial Model'] = r2_score(predictions['Actual Target'], predictions['Polynomial Model'])\nr2_scores['Ridge Model'] = r2_score(predictions['Actual Target'], predictions['Ridge Model'])\nr2_scores['Lasso Model'] = r2_score(predictions['Actual Target'], predictions['Lasso Model'])\n\n# Print the R2 scores\nfor model, score in r2_scores.items():\n    print(f\"R2 score for {model}: {score:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model that includes all the features without transformation is the model that works better. On the other side, model where were transform one feature (interaction and polynomial) are the ones that shows the wors behaviour.","metadata":{}},{"cell_type":"markdown","source":"### B. Adjusted coefficient of determination $R^2$\n\n**Why to use it?**: When we add more predictiors to the model, $R^2$ always increase (or at least doesn´t decrease). The problem is that adjusted is not as inerpretable than $R^2$.\n\nFormula looks like follows:\n\n$$\nR^2 = 1 - \\left( \\frac{SS_{\\text{res}}}{SS_{\\text{total}}} \\right) \\cdot \\left( \\frac{N-1}{N-K-1} \\right)\n$$\n\nWhere:\n\n* $SS_{\\text{res}}$ and $SS_{\\text{total}}$ are the same as in $R^2$.\n* **N**: Number of observations in the data.\n* **K**: Number of features used in the model (excluding the intercept)\n\nThe second part of the formula is the part that adjust for Degrees of Freedom.","metadata":{}},{"cell_type":"markdown","source":"# 5. Hypothesis tests\n\nTwo aproximations:\n\n* A. See if the model proposed is better than the null model.\n* B. Test if a particular coefficient is statistical different from 0\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### A. Compare with the null model\n\nThe null model (baseline model) is that one that no use any feature to predict the target.\n\nThe null hypothesis is that there are no relation between the features and the target.\n\n**Null model**: Consist in that model that predict using only the mean or the median\n\n$$\n\\hat{y_i} = \\bar{y_i}:\n$$\n\nWhere:\n\n* $\\hat{y_i}$: Prediction.\n* $\\bar{y_i}$: Mean of the target values in the training set.\n\nWe can follow three strategies:\n\n#### a. Difference in $R^2$\n\nCompare the values of the coefficient of determination between the null model and an alternative model. Following this we are going to know what model works better, but we can ensure if one works significatilly better than the other. \n\n#### b. F-test for nested models (Extra Sum of Squares Test)\n\nThis approach is used when the alternative modell is equal to the null model plus some additional predictors.\n\nWe calculate if the explained variance in the full modell is significantly greater than in the null model relative to the increase in degrees of fredoon (number of predictors).\n\nFormula:\n\n$$\nF = \\frac{\\frac{(R^2_{\\text{full}} - R^2_{\\text{reduced}})}{p}}{\\frac{(1 - R^2_{\\text{full}})}{(n - p - 1)}} \n$$\n\n\nWhere:\n* $R^2_{\\text{full}}$ is the R-squared of the full model (with the additional predictors).\n* $R^2_{\\text{reduced}}$ is the R-squared of the reduced model (null model).\n* $p$ is the number of extra predictors added to the reduced model to get the full model.\n* $n$ is the total number of observations.\n\n### c. Information Criteria (AIC\\BIC)\n\n* **AIC**: Akaike Information Criterion. Focus on selecting the model that most closely approximates the truth. Disadvantage: It can favor mode complex model (risk of overfitting). Use when all model to compare have the same complexity or when the models are nested.\n* **BIC**: Bayesian Information Criterion. Strong penalty for complexity, often favoring simpler model especially as sample size grows. Preferable when dealing with very large datasets.\n\n**Characteristics**:\n\n* Meassure the complexity of the model penalizing unnecessary complexity (too much predictors). \n* Lowe values of AIC and BIC means a better model.\n* A significant drop in AIC and BIC values when move from null model to the alternative model can suggest that the increase in the model complexity is justified.\n* Interesting when you are interested in model selection or balancing models.\n\n**Interpretation**: There is no way to test if the improvements is significative. However, there is a rule of thumb. We can apply for both, AIC and BIC:\n\n* Diferences 2: No siginificative.\n* Differences between 2 and 6: Some evidence against the model with the higher value of the criterion. This means the model with the lower AIC\\BIC is better.\n* Differences between 6 and 10: Strong evidence that the model with the lower criterios is better.\n* Difference > 10. Very strong evidence.\n\n### Now we are going to implement these three approachs in Python:\n\n#### a.  Difference in $R^2$\nWe are going to create the null value and compare with the other models we have, then we use the F-test approach and finally the Information Criteria approach.","metadata":{}},{"cell_type":"code","source":"# Empty model: Always predict the mean\nmean_regressor = DummyRegressor(strategy='mean')\nmean_regressor.fit(df_train_features.sort_index(), df_train_target.sort_index())\ntarget_pred_mean = mean_regressor.predict(df_train_target.sort_index())\nprint(target_pred_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we compute $R^2$ for our nul model:","metadata":{}},{"cell_type":"code","source":"r2_null_model = r2_score(df_train_target, target_pred_mean)\nprint(\"R² del Modelo Vacío:\", r2_null_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now compare the $R^2$ of each model:","metadata":{}},{"cell_type":"code","source":"# Predictions as df\ndf_r2_scores = pd.DataFrame(list(r2_scores.items()), columns=['Model', 'R-squared'])\n\n# Add the new column\nnull_model_row = pd.DataFrame({'Model': ['Null Model'], 'R-squared': [r2_null_model]})\ndf_r2_scores = pd.concat([df_r2_scores, null_model_row], ignore_index=True)\n\nprint(df_r2_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the all the models predict better than the null model.\n\n#### b. F-test for nested models (Extra Sum of Squares Test)\n\nIn stat model we need to include a constant if we want an intercept in the model.","metadata":{}},{"cell_type":"code","source":"# Creare a df for the null model. As we don´t have predictors, only a constant is present.\nnull_features   = sm.add_constant(pd.DataFrame(index=df_train_features.index)).sort_index()\nfull_features   = sm.add_constant(df_train_features).sort_index()\n\n#reduce_model = sm.add_constant(X_reduced)\nprint(\"Null features:\\n\", null_features.tail(5), \"\\nMultiple Regression features:\\n\", full_features.tail(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use OLS to fit each model:","metadata":{}},{"cell_type":"code","source":"# Fit the models using OLS\nmodel_null = sm.OLS(df_train_target.sort_index(), null_features.sort_index()).fit()\nmodel_full = sm.OLS(df_train_target.sort_index(), full_features.sort_index()).fit()\n\n# Summary of the models\nprint(\"Null Model Summary:\")\nprint(model_null.summary())\nprint(\"\\nFull Model Summary:\")\nprint(model_full.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we compare if full model is nested within null model:","metadata":{}},{"cell_type":"code","source":"f_test = model_full.compare_f_test(model_null)\nprint(\"\\nF-test result:\", f_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results**:\n\n* **F statistic value**: 65.713. This is the calculated F-statistic for the test. It represents the ratio of the model fit improvement per added predictor to the error of the larger model (including all predictors). A higher F-statistic indicates that the additional predictors in the full model provide a significant improvement in fit over the reduced model.\n* **P-value**: 5.379-116 = <0.001. Probability under the null hypothesis of observing the F-statistic, or one more extreme, given that the null hypothesis is true. Here, the null hypothesis typically states that the additional predictors in the full model do not improve the fit of the model meaningfully compared to the reduced model. A very small p-value, as in this case (which is virtually zero), strongly suggests rejecting the null hypothesis. This means that the additional predictors do significantly improve the model.\n* **Degrees of Freedom**: 11.0. This number represents the degrees of freedom associated with the numerator of the F-statistic, which is generally equal to the number of additional parameters added to the reduced model to get the full model. Here, 11 extra parameters were added to the reduced model to form the full model.\n\n**Overall conclusion**: The features in the full model improve the prediction VS the null model.","metadata":{}},{"cell_type":"markdown","source":"#### c. Information criteria (AIC/BIC)\n\nSame steps as in b to obtain model_null and model_full. As we have these step done yet, we pass to see the AIC and BIC criterials in each model:","metadata":{}},{"cell_type":"code","source":"print(\"Null model AIC:\", round(model_null.aic, 3), \"Full model AIC:\", round(model_full.aic, 3))\nprint(\"Null model BIC:\", round(model_null.bic, 3), \"Full model BIC:\", round(model_full.bic, 3))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We compute Deltas (differences) to interpretate the results:","metadata":{}},{"cell_type":"code","source":"# Diferences:\ndelta_aic = model_null.aic - model_full.aic\ndelta_bic = model_null.bic - model_full.bic\n\nprint(\"Delta AIC:\", round(delta_aic), \"\\nDelta BIC:\", round(delta_bic))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the differences are large so we can conclude the full model is better making predictions than the null model.","metadata":{}},{"cell_type":"markdown","source":"### B. Test if a particular coefficient is statistical different from 0\n\nThis approach let as know each predictor in the model separately. We can test if a particular predictor have a statistically significant relationship with the target. If the coefficient is statistically different from 0, then wecan conclude there is a relationship.\n\nScikit-learn package has no a direct way to compute it but we can fit the model with Scikit-Learn and then use the statsmodel from package stats to explore the statistical summary.\n\nWe are going to use the model with all features that we compute in the Multiple linear regression section.","metadata":{}},{"cell_type":"code","source":"print(model2.intercept_)\nprint(model2.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we obtain the statistical summary using statsmodels. This summary includes the standard errors, t-statistics and p-values of the coefficients. We had do this in the previous section so:","metadata":{}},{"cell_type":"code","source":"print(model_full.summary())","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table above we have a list with all the features informations as well as the intercept.\n\n**Coefficients statistical significative** at 0.05 level:\n\n* Volatile acidity, chlorides,, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\n# 6. Standardised coefficients\n\nIt is usual that our variables (target) have different scales. The unirs of measurement influence on the refression coefficients.\n\n\n**Standardised coefficients**: Coefficients obtained when convert all the variables to standard scores ($z-score$) **BEFORE** run the regression. When we standarized we make the variable have mean 0 and standard deviation 1. \nWhen we standarized we make the variable have mean 0 and standard deviation 1. \n\n**Reasons to use**:\n\n* It is specially important when we want to compare the strenght of different predictors (features).It is specially important when we want to compare the strenght of different predictors (features). An absolute value of $\\beta$ means a stronger relationship with the target.\n* Coefficients of standardized variables represent the change in the dependent variable for a one standard deviation change in the predictor, making it easier to compare the effects of different variables.Coefficients of standardized variables represent the change in the dependent variable for a one standard deviation change in the predictor, making it easier to compare the effects of different variables.\n* Moreover, when we use regularization, standarization is crital because it penalize the coefficients base on their size. \n* Acelerate convergence in algorithms that use gradient descent (like logistic regression or when using regularitation).\n\nWe will see how to do this in python step by step:\n\nStandardize the VariablesStandardize the Variables:","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\ndf_train_features_stand = scaler.fit_transform(df_train_features)\nprint(df_train_features_stand)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use StandarScaler() it is necessary transform our data into an array. We need to recover the index and the columns names before made the regression","metadata":{}},{"cell_type":"code","source":"df_train_features_stand = pd.DataFrame(df_train_features_stand, \n                                       index=df_train_features.index, \n                                       columns=df_train_features.columns).sort_index()\n\n# Print the standardized features with the DataFrame structure\nprint(df_train_features_stand.head(11))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create and fit the model:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression_stand = LinearRegression()\n\n# Fit the model\nmodel_stand = regression_stand.fit(df_train_features_stand.sort_index(),\n                                   df_train_target, \n                                   ) # Features first then target\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can print the intercept and the coefficients:","metadata":{}},{"cell_type":"code","source":"# Show the equation values:\nprint(\"Intercept:\", model_stand.intercept_)\n\n# New way to obtain the coefficients with names and round them to three decimals:\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, model_stand.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have all standarized, we are going to interpretate the intercept and coefficients:\n\n* **Intercept**. (5.625). Expected value when all the predictors (features) are 0. This is the same to say that is the value predicted when all the coefficients take their means. If we see above, this is the predicted value in the null model.\n\n* **Fixed Acidity**. (0.035): A one standard deviation increase in fixed acidity is associated with a 0.035 standard deviation increase in the response variable.\n* **Volatile Acidity**. (-0.185): A one standard deviation increase in volatile acidity leads to a 0.185 standard deviation decrease in the response variable, suggesting a negative relationship.\n* **Citric Acid**. (-0.029):A one standard deviation increase in citric acid results in a 0.029 standard deviation decrease in the response variable.\n* **Residual Sugar**. (0.016): A one standard deviation increase in residual sugar is associated with a 0.016 standard deviation increase in the response variable.\n* **Chlorides**. (-0.065): A one standard deviation increase in chlorides results in a 0.065 standard deviation decrease in the response variable.\n* **Free Sulfur Dioxide**. (0.056): A one standard deviation increase in free sulfur dioxide is linked to a 0.056 standard deviation increase in the response variable.\n* **Total Sulfur Dioxide**. (-0.112): A one standard deviation increase in total sulfur dioxide leads to a 0.112 standard deviation decrease in the response variable.\n* **Density**. (-0.023): A one standard deviation increase in density results in a 0.023 standard deviation decrease in the response variable.\n* **pH**. (-0.082): A one standard deviation increase in pH is associated with a 0.082 standard deviation decrease in the response variable.\n* **Sulphates**. (0.145): A one standard deviation increase in sulphates leads to a 0.145 standard deviation increase in the response variable.\n* **Alcohol**. (0.317): A one standard deviation increase in alcohol is associated with a 0.317 standard deviation increase in the response variable, making it the most impactful predictor among those listed.\n","metadata":{}},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 5.625 + 0.035 · x_1 -0.185 · x_2 - 0.029 · x_3 + 0.016 · x_4  - 0.065 · x_5 + 0.056 · x_6 - 0.112 · x_7 - 0.023 · x_8 - 0.082 · x_9 + 0.145 · x_{10} + 0.317 · x_{11} \n$$\n\n\nAs we see,  coefficient take the same signe (positive/negative) that in other models\n\nIf we replace the features for our first case we obtain this (use the standarized features):","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features_stand.sort_index().iloc[0:1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The equation for our first case is like:\n\n$$\n\\hat{y} = 5.625 + 0.035 · (-0.302) -0.185 ·  (- 1.922) -0.029 · (-1.381) + 0.016 · 0.058  - 0.065 · 0.259 + 0.056 · 0.896 - 0.112 · 0.613 - 0.023 · 0.025 - 0.082 · (-0.728) + 0.145 · 0.155 + 0.317 · (-0.578) \\approx  5.883\n\n\n$$\n","metadata":{}},{"cell_type":"code","source":"model_stand_y = pd.DataFrame(model_stand.predict(df_train_features_stand), \n                      index=df_train_features_stand.index, \n                      columns=['Standarized model'])\n\nprint(\"Quality predicted value for the first case:\", model_stand_y.sort_index().head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}