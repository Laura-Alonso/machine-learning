{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8204,"sourceType":"datasetVersion","datasetId":4458}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INTRODUCTION TO THE NOTEBOOK\n\n## Aim\n\nIn this notebook, we will explore the principal characteristics of the linear regression models. This document is designed to emphasize technical details over the typical step-by-step procedure for implementing a regression model. Consequently, the organization of sections does not follow the traditional sequence. A complementary document will be provided, featuring a practical example to cover it thoroughly.\n\nPlease feel free to suggest any corrections, modifications, or improvements. **Your feedback is greatly appreciated**.\n\n## Programming Language\n\nThe code in the following sections is developed using Python (v. 3.10.13). The versions of the packages used are:\n\n* **Pandas**: '2.2.2' \n* **scikit-learn**: '1.2.2' \n* **Matplotlib**: '3.7.5'\n* **Numpy**: '1.26.4'\n* **Statsmodels**: '0.14.1'","metadata":{}},{"cell_type":"code","source":"import pandas as pd                                  # Read csv\nfrom sklearn.linear_model  import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures # interaction effect, no linear models\nfrom sklearn.linear_model  import Ridge, RidgeCV, Lasso, LassoCV # regularizationy, best value of alpha Ridge, reduce features, best value of alpha Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.dummy import DummyRegressor\nimport matplotlib\nfrom sklearn.preprocessing import StandardScaler # standardization\nimport matplotlib.pyplot as plt                  # plot\nimport numpy as np\nimport statsmodels.api as sm\n\n\n# Check Python and packages version.\nimport sys\nfrom sklearn import __version__ as sklearn_version # Only needed for sklearn\n\nversions = {\n    \"Python\": sys.version.split(\" \")[0],  # Simplifying to just the version number\n    \"Pandas\": pd.__version__,\n    \"scikit-learn\": sklearn_version,\n    \"Matplotlib\": matplotlib.__version__,\n    \"Numpy\": np.__version__,\n    \"Statsmodels\": sm.__version__\n}\n\n# print(versions)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:24.556458Z","iopub.execute_input":"2024-05-10T14:25:24.556873Z","iopub.status.idle":"2024-05-10T14:25:27.381800Z","shell.execute_reply.started":"2024-05-10T14:25:24.556837Z","shell.execute_reply":"2024-05-10T14:25:27.380542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Database\n\nThroughout this notebook, we will use the \"Red Wine Quality\" database. You can find a complete description of this dataset by following this link: [Red Wine Quality Dataset Description](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009).\n\n* All the variable in this database are quantitative.\n* The target variable is \"quality\", which ranges from 0 to 10.\n\nNow, we are going to upload the data and separate target from features. There are two important things that this notebook ommit:\n\n* Split data in training and test. \n* Explore and clean the data.\n* Check the assumptions of the model.\n\nYou can see it in practise in the complementary example.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Read csv and see the chaacteristics\ndf = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nprint(\"Number of rows:\", len(df))\nprint(df.head(5))\n\n# Separate target and features:\n\ndf_target   = df[['quality']]      # Use doble [[ ]] to avoid transform in serie (because is only one column)\ndf_features = df.drop('quality', axis=1)\n\nprint(\"\\n\")\nprint(\"The compleate df has\", len(df),          \"cases and\", df.shape[1],          \"columns\")\nprint(\"df_target has\",        len(df_target),   \"cases and\", df_target.shape[1],   \"column\")\nprint(\"df_features has\",      len(df_features), \"cases and\", df_features.shape[1], \"columns\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.383577Z","iopub.execute_input":"2024-05-10T14:25:27.384024Z","iopub.status.idle":"2024-05-10T14:25:27.433870Z","shell.execute_reply.started":"2024-05-10T14:25:27.383997Z","shell.execute_reply":"2024-05-10T14:25:27.432693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the data**\n\nWe have 1,599 entries (cases,rows), one target variable (\"quality\") and 11 features.","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction to Linear Regression\n\nLinear regression is a machine learning algorithm classified under \"supervised learning\" techniques.\n\nThe **aim** of linear regression algorithm is to predict one variable (VD, also known as *target* or *y*) using one or more features (VI, *predictors* or *$x_i$*).\n\nThere are two type of models depending on the number of features involve:\n\n* **Simple Linear Regression**. The model only one feature to predict the target:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\epsilon\n$$\n\n* **Multiple Linear Regression**. This model use two or more features to predict the target.\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 +  \\hat{\\beta}_2 x_2 + ... + \\hat{\\beta}_n x_n + \\epsilon\n$$\n\nWhere:\n\n* $ \\hat{y} $: Is the target we predict.\n* $ \\hat{\\beta}_0 $: Is the *intercept* or *bias*.\n* $ \\hat{\\beta}_1, \\hat{\\beta}_2, ... $: Are the *coefficients* (*weights*, *effect*) associated with each feature. Is the effect of one feature on the target.\n* $ \\epsilon $: Represents the error term.\n\n\n**Note**: When discussing estimations (like parameter estimations or model residuals) or predictions, we use the hat notation, as in $\\hat{y}$.","metadata":{}},{"cell_type":"markdown","source":"## A. Model 1: Simple Linear Regression Model\n\nThis model uses only one feature to predict the \"quality\" of the wine, which is our target variable stored in *df_target*. \n\n\nThe predictor we will use is the \"alcohol\" level, currently housed within *df_features* along with other features. To simplify our analysis, we will first extract this specific feature into a new object dedicated solely to it.","metadata":{}},{"cell_type":"code","source":"# Select only the feature alcohol\ndf_feature_one = df_features[['alcohol']]\n\n# Print results\nprint(\"Target quality \\n\", df_target.sort_index().head(5), \"\\n\")\nprint(\"Feature alcohol \\n\", df_feature_one.sort_index().head(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.435279Z","iopub.execute_input":"2024-05-10T14:25:27.436004Z","iopub.status.idle":"2024-05-10T14:25:27.446572Z","shell.execute_reply.started":"2024-05-10T14:25:27.435965Z","shell.execute_reply":"2024-05-10T14:25:27.445259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll construct the model to establish the relationship between \"quality\" and \"alcohol\" content. Here's the step-by-step process:\n\n* **Create the model**. In this case, Linear Regression model.\n* **Fit the model**. Train the model with our data. During this process, the model will adjust its parameters to best fit the relationship between quality and alcohol content. As a result, we'll obtain the coefficients and the intercept.\n* **Display Coefficients and Intercept**: After fitting the model, we'll showcase the numerical values for the intercept and coefficients. These values provide insights into the baseline quality (intercept) and the magnitude of influence (coefficients) that alcohol content has on the quality of the product.","metadata":{}},{"cell_type":"code","source":"# Create SLRM and fit it\nregression1 = LinearRegression()\n\n# Fit the model\nmodel1 = regression1.fit(df_feature_one.sort_index(),   # Features first then target\n                         df_target.sort_index()) \n\n# Show the results (round to three decimals):\nprint(\"Intercept:\",     model1.intercept_[0].round(3))\nprint(\"Coefficients :\", model1.coef_[0][0].round(3))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.450135Z","iopub.execute_input":"2024-05-10T14:25:27.450673Z","iopub.status.idle":"2024-05-10T14:25:27.474232Z","shell.execute_reply.started":"2024-05-10T14:25:27.450630Z","shell.execute_reply":"2024-05-10T14:25:27.472928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whith this information, we can construct the equation of the model:\n\n$$\n\\hat{y} = 1.875 + 0.361 · x_1 + \\epsilon\n$$\n\nThe error $ \\epsilon $ epresents the disparity between the actual value of  $ y $ and the predicted value $ \\hat{y} $. We can only ascertain this value when we possess the actual result, not during prediction.\n\nNevertheless, we can substitute the feature value into the equation. For our initial scenario, this value is:","metadata":{}},{"cell_type":"code","source":"print(\"Value of alcohol feature for our first case :\", df_feature_one.iloc[0:1])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.476309Z","iopub.execute_input":"2024-05-10T14:25:27.477211Z","iopub.status.idle":"2024-05-10T14:25:27.484931Z","shell.execute_reply.started":"2024-05-10T14:25:27.477137Z","shell.execute_reply":"2024-05-10T14:25:27.483934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The equation for the first case is:\n\n$$\n\\hat{y} = 1.875 + 0.361 · 9.4 \n$$\n\nSo, the predicted value of our first case is:\n\n$$\n\\hat{y} = 1.830 + 0.364 · 10 \\approx 5.268\n$$\n\nWe can achieve this using our script:","metadata":{}},{"cell_type":"code","source":"model1_y = pd.DataFrame(model1.predict(df_feature_one.sort_index()), \n                      index=df_feature_one.index, \n                      columns=['Simple Model'])\n\nprint(\"Quality predicted value for the frst case:\", model1_y.sort_index().round(3).head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.486081Z","iopub.execute_input":"2024-05-10T14:25:27.486447Z","iopub.status.idle":"2024-05-10T14:25:27.503632Z","shell.execute_reply.started":"2024-05-10T14:25:27.486419Z","shell.execute_reply":"2024-05-10T14:25:27.502333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are going to plot our model, putting in relation the predicted and real values of the \"quality\" of wine. The elements are:\n\n* **x-axis**: Predicted value of the target.\n* **y-axis**: Real value of the target.\n* **Dots**: Represent each wine \"quality\" (predicted vs real). The more filled dots are, the more cases concentrated on that relation.\n* **Predicted line**. Dashed red line. Represents the predictions if the model was perfect. The closer the dots to the line, the better predictions are made.\n* **Axis limits**. Ranging between 0 to 10, which are the minimum and maximum theoretical values of the \"quality\".\n\nThe interpretation that we can make is:\n\n* The real values of the target range between 3 and 9, while the predicted values range between 5 and 7 (approximately). So not all the variability is captured by the model.\n* Dots are closer to the predicted line in the medium values. This means that our model is not good at predicting when the \"quality\" of wine is lower or higher. However, overall, the model doesn't predict very well.\n\nYou can see the code to make this plot and the output of it:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.scatter(df_target.sort_index(),     # Real value\n            model1_y.sort_index(),      # Predicted\n            color='lightskyblue',\n            edgecolor = \"black\",\n            label='Predicted vs Real',\n            alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model1_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.505284Z","iopub.execute_input":"2024-05-10T14:25:27.505836Z","iopub.status.idle":"2024-05-10T14:25:27.913458Z","shell.execute_reply.started":"2024-05-10T14:25:27.505807Z","shell.execute_reply":"2024-05-10T14:25:27.912078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. Model 2: Multiple Linear Regression Model\n\nThis model uses more than one feature to predict the \"quality\" of the wine, which is our target variable stored in df_target.\n\nThe predictors we will use are all the available features (11 variables), storesd in df_features.\n\nThe steps are identical to those in model 1.","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression2 = LinearRegression()\n\n# Fit the model\nmodel2 = regression2.fit(df_features.sort_index(), \n                         df_target.sort_index()) \n\n# Show the equation values:\nprint(\"Intercept :\",    model2.intercept_ [0].round(3))\n\ncoefficient_dict = dict(zip(df_features.columns,\n                            model2.coef_[0]))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.914976Z","iopub.execute_input":"2024-05-10T14:25:27.915451Z","iopub.status.idle":"2024-05-10T14:25:27.940883Z","shell.execute_reply.started":"2024-05-10T14:25:27.915409Z","shell.execute_reply":"2024-05-10T14:25:27.939440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whith this information we can build the equation of the model:\n\n$$\n\\hat{y} = 21.956 + 0.025 · x_1 - 1.084 · x_2 - 0.183 · x_3 + 0.016 · x_4 - 1.874 · x_5 + 0.004 · x_6 - 0.003 · x_7 - 17.881 · x_8 - 0.414 · x_9 + 0.916 · x_{10} + 0.276 · x_{11}\n$$\n\nWe can see that we have negative and positive values in the coeficients. We interpret them as follows:\n\n* **Positive**: A high value in that feature increases the value of the target.\n* **Negative**: A high value in that feature decreases the value of the target.\n\nIf we replace the features for our first case, we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_features.iloc[0:1].transpose().round(3))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.943069Z","iopub.execute_input":"2024-05-10T14:25:27.944036Z","iopub.status.idle":"2024-05-10T14:25:27.960636Z","shell.execute_reply.started":"2024-05-10T14:25:27.943983Z","shell.execute_reply":"2024-05-10T14:25:27.954319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the first case the final equation looks like:\n\n$$\n\\hat{y} = 21.956 + 0.025 · 7.4 - 1.084 · 0.7 - 0.183 · 0 + 0.016 · 1.9 - 1.874 · 0.076 + 0.004 · 11 - 0.003 · 34 - 17.881 · 0.998 - 0.414 · 3.510 + 0.916 · 0.560 + 0.276 · 9.4\n$$\n\nAnd the quality predicted value :\n\n$$\n\\hat{y} \\approx 5.021\n$$\n\nWe can obtain this result using our script (discrepancies might be due to differences in decimal precision)","metadata":{}},{"cell_type":"code","source":"model2_y = pd.DataFrame(model2.predict(df_features.sort_index()), \n                        index=df_features.index, \n                        columns=['Multiple Model'])\n\nprint(\"Quality predicted value for the frst case:\", model2_y.sort_index().round(3).head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:27.965580Z","iopub.execute_input":"2024-05-10T14:25:27.966374Z","iopub.status.idle":"2024-05-10T14:25:27.997979Z","shell.execute_reply.started":"2024-05-10T14:25:27.966339Z","shell.execute_reply":"2024-05-10T14:25:27.995662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, we represent the relation between prediction and real value using a plot. We plot the result from model 1 and model 2 side by side to compare both models.\n\nThere are no large differences between both models. Model 2 performe a slightly better than model 1. We can affirm that because the dots are more close to the red line in model 2 (look when real wine \"quality\" is 5).\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n\n# Plot model 1\nplt.subplot(1, 2, 1)\n\nplt.scatter(df_target.sort_index(),     # Real value\n            model1_y.sort_index(),      # Predicted\n            color='lightskyblue',\n            edgecolor = \"black\",\n            label='Predicted vs Real',\n            alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model1_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Simple Regression: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\n\n# Plot model 2\nplt.subplot(1, 2, 2)\n\nplt.scatter(df_target.sort_index(),     # Real value\n                model2_y.sort_index(),      # Predicted\n                color='lightskyblue',\n                edgecolor = \"black\",\n                label='Simple Regression: Predicted vs Real',\n                alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model2_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Multiple Regression: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.001880Z","iopub.execute_input":"2024-05-10T14:25:28.005089Z","iopub.status.idle":"2024-05-10T14:25:28.676800Z","shell.execute_reply.started":"2024-05-10T14:25:28.005031Z","shell.execute_reply":"2024-05-10T14:25:28.675499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Features modifications \n\n## A. Interacction effect\n\nSometimes, individual features may not exhibit a significant effect on the target variable by themselves, but their combination does. We can account for this type of effect by including an interaction term in the model.\n\nIncluding interaction terms in the model allows for the possibility that the relationship between predictors and the target variable is not additive ($ \\hat{\\beta}_1 \\cdot x_1 + \\hat{\\beta}_2 \\cdot x_2 $), but varies depending on the levels of other predictors ($ \\hat{\\beta}_3 \\cdot x_1 \\cdot x_2 $).\n\nImagine we have an original model with two features: $x_1$ and $x_2$. As explained in the introduction, the model will take this form:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\epsilon\n$$\n\nIf we suspect there is an interaction between these two features, we can add the interaction effect as follows:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1 x_2 + \\epsilon\n$$\n\nSome relevant points:\n\n* We can include as many interaction effects as we want if they are interesting. However, we should consider the relevance of including them because we run the risk of overfitting our model.\n* An interaction effect could involve more than two features. For example, ($ \\hat{\\beta}_3 · x_1 · x_2 · x_3 $). If you decide this is interesting in your case, be cautious about overfitting and the difficulty of interpreting the interaction.\n* It is not necessary to include interactions for all the features involved in the model. If we have a model with three features and we think only the interaction between $ x_1 $ and $ x_2 $ is ineresting, then we can create a model like this: \n\n$$ \n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + \\hat{\\beta}_4 x_1 x_2 + \\epsilon \n$$\n\nNow, let's learn how to implement this in Python.\n\nAs before, our target variable is \"quality\". For predictors, we'll include all the features, along with the interaction between \"free sulfur dioxide\" and \"alcohol\".\n\nFirst, we'll create the interaction term:\n","metadata":{}},{"cell_type":"code","source":"interaction = PolynomialFeatures(\n    degree=2,                    # How many features combinations we want to create\n    include_bias=False,          # Exclude the bias term \n    interaction_only=True        # Only include interaction effects (not x^2, x^3, etc.)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.678764Z","iopub.execute_input":"2024-05-10T14:25:28.679206Z","iopub.status.idle":"2024-05-10T14:25:28.684814Z","shell.execute_reply.started":"2024-05-10T14:25:28.679165Z","shell.execute_reply":"2024-05-10T14:25:28.683432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have to transform and fit the interaction between free sulfur dioxide and alcohol:","metadata":{}},{"cell_type":"code","source":"features_interaction = interaction.fit_transform(df_features[['free sulfur dioxide', 'alcohol']])\n\n# This is not necessary but is good to see the results. Transform the array into a dataframe:\nfeatures_interaction = pd.DataFrame(features_interaction,\n                                      columns = interaction.get_feature_names_out(['free sulfur dioxide', 'alcohol']),\n                                      index=df_features.index  # This line ensures the index is carried over. Example, index 10 doesn´t should appear\n                                   )\nprint(features_interaction.sort_index().head(1))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.686864Z","iopub.execute_input":"2024-05-10T14:25:28.687380Z","iopub.status.idle":"2024-05-10T14:25:28.708859Z","shell.execute_reply.started":"2024-05-10T14:25:28.687341Z","shell.execute_reply":"2024-05-10T14:25:28.707371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we observed, the interaction term includes both the simple features (since degree = 2 encompasses degree = 1, which represents the simple effects) and the interaction itself. We are specifically interested in the interaction between \"free sulfur dioxide\" and \"alcohol\". Therefore, we extract only that column and add it to our original features DataFrame:","metadata":{}},{"cell_type":"code","source":"interaction_column = features_interaction[['free sulfur dioxide alcohol']].sort_index()\nprint(interaction_column.sort_index().head(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.712193Z","iopub.execute_input":"2024-05-10T14:25:28.712708Z","iopub.status.idle":"2024-05-10T14:25:28.725039Z","shell.execute_reply.started":"2024-05-10T14:25:28.712668Z","shell.execute_reply":"2024-05-10T14:25:28.723568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We add the interaction column to our original features DataFrame. To demonstrate that this DataFrame has been modified, we will create a new DataFrame to display the modified version.","metadata":{}},{"cell_type":"code","source":"df_features_interaction = df_features.join(interaction_column, how='outer')\n\nprint(df_features_interaction.sort_index().head(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.726656Z","iopub.execute_input":"2024-05-10T14:25:28.727122Z","iopub.status.idle":"2024-05-10T14:25:28.750050Z","shell.execute_reply.started":"2024-05-10T14:25:28.727079Z","shell.execute_reply":"2024-05-10T14:25:28.748758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have a new DataFrame with all the original features plus one additional column for the interaction term (which is the last column, \"free sulfur dioxide alcohol\").\n\n**Important Note**: In this notebook, we are only utilizing the global database without splitting it into train and test sets. However, if we were to conduct such a split, the calculation of the interaction should be performed before splitting the data.\n\nWith our final DataFrame, df_features_interaction, we can proceed to fit the model and obtain the intercept and coefficients:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression3 = LinearRegression()\n\n# Fit the model\nmodel3 = regression3.fit(df_features_interaction.sort_index(), \n                         df_target.sort_index()) # Features first then target\n\n# Show the equation values:\nprint(\"Intercept :\",    model3.intercept_[0].round(3))\n\ncoefficient_dict = dict(zip(df_features_interaction.columns,\n                            model3.coef_[0]))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:44:01.058557Z","iopub.execute_input":"2024-05-10T14:44:01.058996Z","iopub.status.idle":"2024-05-10T14:44:01.075905Z","shell.execute_reply.started":"2024-05-10T14:44:01.058964Z","shell.execute_reply":"2024-05-10T14:44:01.074367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create the equation of the model:\n\n$$\n\\hat{y} = 21.055 + 0.024 · x_1 - 1.082 · x_2 - 0.170 · x_3 + 0.019 · x_4 - 1.840 · x_5 - 0.018 · x_6 - 0.003 · x_7 - 16.610 · x_8 - 0.420 · x_9 + 0.903 · x_{10} + 0.245 · x_{11} + 0.002 · x_{6} · x_{11}\n$$\n\nIf we substitute the features for our first case, we obtain the following:","metadata":{}},{"cell_type":"code","source":"# Notice that we use df_features_interaction\nprint(\"Value of all features for our first case :\", df_features_interaction.iloc[0:1].transpose().round(3))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.778079Z","iopub.execute_input":"2024-05-10T14:25:28.778889Z","iopub.status.idle":"2024-05-10T14:25:28.794535Z","shell.execute_reply.started":"2024-05-10T14:25:28.778841Z","shell.execute_reply":"2024-05-10T14:25:28.793141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We replace with the features information:\n\n$$\n\\hat{y} = 21.055 + 0.024 · 7.4 - 1.082 · 0.7 - 0.170 · 0 + 0.019 · 1.9 - 1.840 · 0.076 - 0.018 · 11 - 0.003 · 34 - 16.610 · 0.998 - 0.420 · 3.510 + 0.903 · 0.560 + 0.245 · 9.4 + 0.002 · 103.4 \\approx 5.036\n$$\n\nUsing Python:","metadata":{}},{"cell_type":"code","source":"model3_y = pd.DataFrame(model3.predict(df_features_interaction.sort_index()), \n                        index=df_features_interaction.index, \n                        columns=['Interaction Model'])\n\nprint(\"Quality predicted value for the frst case:\", model3_y.sort_index().round(3).head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.796582Z","iopub.execute_input":"2024-05-10T14:25:28.797063Z","iopub.status.idle":"2024-05-10T14:25:28.820602Z","shell.execute_reply.started":"2024-05-10T14:25:28.797025Z","shell.execute_reply":"2024-05-10T14:25:28.819161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are set to examine the relationship between the actual \"quality\" and the predicted \"quality\" of our model. We include two plots with two models: multiple regresion and interaction. Both include the 11 original features, but the second model also incorporates an interaction term. Despite these differences in model configuration, our findings indicate that there are no discernible differences in their predictions.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n\n# Plot model 2\nplt.subplot(1, 2, 1)\n\nplt.scatter(df_target.sort_index(),     # Real value\n            model2_y.sort_index(),      # Predicted\n            color='lightskyblue',\n            edgecolor = \"black\",\n            label='Predicted vs Real',\n            alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model1_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Multiple Regression: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\n\n# Plot model 3\nplt.subplot(1, 2, 2)\n\nplt.scatter(df_target.sort_index(),     # Real value\n                model3_y.sort_index(),      # Predicted\n                color='lightskyblue',\n                edgecolor = \"black\",\n                label='Model with interaction term: Predicted vs Real',\n                alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model3_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Multiple Regression with interaction term: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:14:15.309426Z","iopub.execute_input":"2024-05-10T15:14:15.309817Z","iopub.status.idle":"2024-05-10T15:14:15.961391Z","shell.execute_reply.started":"2024-05-10T15:14:15.309790Z","shell.execute_reply":"2024-05-10T15:14:15.960443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. Nonlinear relationships\n\nIn linear models, we typically expect the relationship between the target and features to be constant. However, when this assumption doesn't hold true, the relationship is deemed nonlinear. For instance, consider a student preparing for an exam. Generally, more hours of study lead to higher scores. However, this relationship is not linear. Spending one hour studying doesn't necessarily result in a score of one on the test, nor does studying for seven hours guarantee a score of seven. Instead, the relationship between the target (\"score\") and the feature (\"number of hours of study\") is exponential.\n\nA model that incorporates a nonlinear effect in a feature accounts for this aspect:\n\n$$\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 +  \\hat{\\beta}_2 x_1^2 + ... + \\hat{\\beta}_n x_n + \\epsilon\n$$\n\nWhere:\n\n* $ x_1 $: Represents the feature with a linear effect.\n* $ x_1^2 $: Represents the same feature with a nonlinear association. The exponent $ ^2 $ indicates the polynomial degree.\n\n**Include only nonlinear relationshiph or both?**:\n\nIf you believe that the relationship between a feature and the target variable is purely nonlinear, then you might only include the nonlinear term in your model. However, in many cases, including both linear and nonlinear terms can provide a more flexible and accurate representation of the relationship.\nAdditionally, including both linear and nonlinear terms can help prevent issues like omitted variable bias, where failing to include relevant variables in the model leads to biased and inconsistent parameter estimates.\n\n\n\nThe approach (Python) is similar to that of incorporating interaction effects. We will create the complete model, including nonlinear effect for the sulfite feature and then create and fit the model.\nWe begin by creating a new feature $ sulphite^2 $, which represents the square of the sulfite feature. This is essentially an interaction between the same feature.\n\n","metadata":{}},{"cell_type":"code","source":"polynomial = PolynomialFeatures(\n    degree=2,                    # We need a interaction with two degrees.\n    include_bias=False,          # Exclude the bias term \n    interaction_only=False       # NEW. Include polynomia effects. In our case only x^2 because we are interesting in 2 degrees.\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:28.822772Z","iopub.execute_input":"2024-05-10T14:25:28.823288Z","iopub.status.idle":"2024-05-10T14:25:28.834514Z","shell.execute_reply.started":"2024-05-10T14:25:28.823249Z","shell.execute_reply":"2024-05-10T14:25:28.832891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have to transform and fit the polynomial effect of \"sulphates\":","metadata":{}},{"cell_type":"code","source":"polynomial_features = polynomial.fit_transform(df_features[['sulphates']].sort_index())\n\n# This is not necessary but is good to see the results. Transform the array into a dataframe:\nfeatures_polynomial = pd.DataFrame(polynomial_features,\n                                   columns = polynomial.get_feature_names_out(),\n                                   index=df_features.index  # This line ensures the index is carried over. Example, index 10 doesn´t should appear\n                                   )\nprint(features_polynomial.sort_index().head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:34:42.011865Z","iopub.execute_input":"2024-05-10T14:34:42.012333Z","iopub.status.idle":"2024-05-10T14:34:42.026751Z","shell.execute_reply.started":"2024-05-10T14:34:42.012300Z","shell.execute_reply":"2024-05-10T14:34:42.025381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only need incorporate the $ sulphates^2 $ column. First, we isolete that column and then we add it to the original feature df (we create a new dataframe for this new data)","metadata":{}},{"cell_type":"code","source":"# Isolate the polynomial feature\npolynomial_column = features_polynomial[['sulphates^2']].sort_index()\n\n# Add to the other features (and create a new features df)\ndf_features_polynomial = df_features.join(polynomial_column, how='outer')\nprint(df_features_polynomial.sort_index().head(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:39:29.556060Z","iopub.execute_input":"2024-05-10T14:39:29.556507Z","iopub.status.idle":"2024-05-10T14:39:29.573826Z","shell.execute_reply.started":"2024-05-10T14:39:29.556470Z","shell.execute_reply":"2024-05-10T14:39:29.572498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ImportantNote**. As we say in the interaction section, if we are going to split the data into training and testing, then this steps should be done before.\n\nWith our final DataFrame, df_features_polynomial, we can proceed to fit the model and obtain the intercept and coefficients:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression4 = LinearRegression()\n\n# Fit the model\nmodel4 = regression4.fit(df_features_polynomial.sort_index(), \n                         df_target.sort_index()) # Features first then target\n\n# Show the equation values:\nprint(\"Intercept :\",    model4.intercept_[0].round(3))\n\ncoefficient_dict = dict(zip(df_features_polynomial.columns,\n                            model4.coef_[0]))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:45:22.579558Z","iopub.execute_input":"2024-05-10T14:45:22.579945Z","iopub.status.idle":"2024-05-10T14:45:22.599835Z","shell.execute_reply.started":"2024-05-10T14:45:22.579919Z","shell.execute_reply":"2024-05-10T14:45:22.596816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 32.52 + 0.013 · x_1 - 0.967 · x_2 - 0.214 · x_3 + 0.017 · x_4 -1.599 · x_5 + 0.003 · x_6 - 0.003 · x_7 - 28.532 · x_8 - 0.662 · x_9 + 3.640 · x_{10} + 0.260 · x_{11} - 1.565 · x_{10}^2\n$$\n\nObserving the polynomial coefficient, it becomes evident that it's negative. This signifies that as the levels of\" sulfates\" increase, the \"quality\" of the wine tends to decrease. Given its polynomial nature, this decline in \"quality\" accelerates, indicating a rapid deterioration with higher \"sulfates\".\n\nUpon substituting the features in our initial case, the resultant trend emerges as follows:","metadata":{}},{"cell_type":"code","source":"# Notice that we use df_features_interaction\nprint(\"Value of all features for our first case :\", df_features_polynomial.iloc[0:1].transpose().round(3))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:02:07.822812Z","iopub.execute_input":"2024-05-10T15:02:07.823268Z","iopub.status.idle":"2024-05-10T15:02:07.832494Z","shell.execute_reply.started":"2024-05-10T15:02:07.823236Z","shell.execute_reply":"2024-05-10T15:02:07.831131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We replace our model with the features information case one:\n\n$$\n\\hat{y} = 32.52 + 0.013 · 7.4 - 0.967 · 0.7 - 0.214 · 0 + 0.017 · 1.9 -1.599 · 0.076 + 0.003 · 11 - 0.003 · 34 - 28.532 · 0.998 - 0.662 · 3.510 + 3.640 · 0.560 + 0.260 · 9.4 - 1.565 · 0.314 \\approx 4.973\n$$\n\nUsing Python:","metadata":{}},{"cell_type":"code","source":"model4_y = pd.DataFrame(model4.predict(df_features_polynomial.sort_index()), \n                      index=df_features_polynomial.index, \n                      columns=['Polinomyal Model'])\n\nprint(\"Quality predicted value for the frst case:\", model4_y.sort_index().head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:06:57.537302Z","iopub.execute_input":"2024-05-10T15:06:57.537696Z","iopub.status.idle":"2024-05-10T15:06:57.553200Z","shell.execute_reply.started":"2024-05-10T15:06:57.537669Z","shell.execute_reply":"2024-05-10T15:06:57.552027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create again the two plots: multiple regression and regression with polynomical term. Again, no differences are detected.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n\n# Plot model 2\nplt.subplot(1, 2, 1)\n\nplt.scatter(df_target.sort_index(),     # Real value\n            model2_y.sort_index(),      # Predicted\n            color='lightskyblue',\n            edgecolor = \"black\",\n            label='Predicted vs Real',\n            alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model1_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Multiple Regression: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\n\n# Plot model 4\nplt.subplot(1, 2, 2)\n\nplt.scatter(df_target.sort_index(),     # Real value\n                model4_y.sort_index(),      # Predicted\n                color='lightskyblue',\n                edgecolor = \"black\",\n                label='Model with interaction term: Predicted vs Real',\n                alpha = 0.3)\n\n# Plot a diagonal line representing perfect predictions and customize\nmax_val = max(np.max(df_target), np.max(model4_y))\nplt.plot([0, max_val], [0, max_val], \n         color='red', \n         linestyle='--', \n         label='Perfect Prediction')\n\nplt.xticks(np.arange(0,10+1, 1))\nplt.yticks(np.arange(0,10+1, 1))\nplt.xlabel('Real \"Quality\"')\nplt.ylabel('Predicted \"Quality\"')\nplt.title('Multiple Regression with polynomical term: Predicted vs Real Values')\nplt.legend()\nplt.grid(color = \"gray\",\n         alpha = 0.3)\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T15:13:12.244414Z","iopub.execute_input":"2024-05-10T15:13:12.244905Z","iopub.status.idle":"2024-05-10T15:13:12.881483Z","shell.execute_reply.started":"2024-05-10T15:13:12.244871Z","shell.execute_reply":"2024-05-10T15:13:12.880218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Optimization Techniques\n\nScikit-learn offers a wide array of estimator options, covering various techniques beyond regression. The image below illustrates the extensive variety included in this package. For more details, you can explore [Scikit-learn estimators](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n\n<div style=\"text-align:center\">\n    <img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"Scikit-Learn estimators\" width=\"700\" height=\"500\">\n</div>\n\n<br>\n\n**Optimization algorithms** are tools used to find the best solution to a problem, typically aiming to minimize or maximize a function. Key concepts include:\n\n* **Objective Function**: The function we aim to optimize.\n* **Minimum and Maximum**: We seek the minimum when optimizing parameters to minimize error and the maximum when maximizing profits.\n* **Stopping Conditions**: Criteria indicating when to cease the search for the optimal solution. For instance, fixed iteration counts or when improvements in the objective function fall below a specific threshold.\n* **Optimal Point**: The input value yielding the minimum or maximum of the objective function.\n* **Gradient**: A vector indicating both the direction and magnitude of the steepest change in a function.\n\n<br>\n\n## A. Residual Sum of Squares (RSS)\n\nThe **Residual Sum of Squares (RSS)** serves as a metric to compute differences between real and estimated values. It's calculated using the formula:\n\n$$\nRSS = \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2\n$$\n\nWhere:\n\n* i: Denotes a specific row.\n* n: Represents the total sample.\n* $y_i$ : Refers to the real value of the target.\n* $\\hat{y}_i$: Refers to the estimated value of the target.\n\nRegularization model includes some penalization into this RSS.\n\n<br>\n\n### a. Ordinary Least Square (OLS)\n\n**Ordinary Least Squares (OLS)** is a method employed to minimize the sum of the squared errors, which represent the differences between predictions and actual values.\n\n* Usefull when the errors follows a normal distribution and there are no outliers in the data.\n* OLS is widely used in linear regression analysis to estimate the coefficients of the linear equation that best fits the observed data points. It provides a \"best fit\" line through the data by determining the coefficients that minimize the sum of the squared residuals.\n\n<br>\n<br>\n\n## B. Gradient Descent\n\n**Gradient Descent** is an iterative optimization process used to find the optimal parameters of a model.\n\n* Is an iterative process. It repeatedly updates the parameters until convergence to minimize the loss function.\n* Use the enterie sample in each iteration to update parameters.\n* Parameter are update in the oposite direction of the gradient of the loss function.\n* Search the local or global minimun.\n* While Gradient Descent might be slower than certain methods for simple linear regression problems, it offers more flexibility and is particularly useful for complex problems with non-linear relationships.\n\n<br>\n<br>\n## C. SGD Regressor (Stochastic Gradient Descent)\n\n**Stochastic Gradient Descent (SGD)** is a variant of Gradient Descent used in machine learning. It seeks to minimize the loss function to update the model parameters.\n\n* Search for the minimun to update the model parameters.\n* Use when you have large samples.\n* Unlike traditional Gradient Descent, SGD utilizes a random sample of the data in each iteration. This characteristic makes it efficient for large training samples, as it processes smaller batches of data at a time.\n\n<br>\n<br>\n\n## D. Regularization\n\nRegularization methods are used to prevent overfitting by adding a penalty term to the loss function. This penalty term encourages the model to learn simpler patterns and helps prevent it from fitting noise in the training data. By optimizing both the original loss function and the regularization term, regularization techniques strike a balance between fitting the training data well and generalizing to unseen data, thus improving the overall performance of the model\n\n**Differences between Lasso and Ridge Regression**:\n\n| | Ridge Regression (L2) | Lasso Regression (L1)|\n|:--:| :--:|:--:|\n|Penalty term| Proportional to the square of the square of the magnitude of the coefficients: $ \\Theta^2 $ | Proportional to the absolute value of the coefficients $ |\\Theta| $. It could drive some coefficients to 0|\n|Objetive| Reduces overfitting and model complexity| Reduces complexity and automatically selects some features (by setting their coefficients to 0. In other words, deleting it)|\n|Predictions| Generally yields better predictions| More interpretable due to feature selection|\n\n**$\\alpha$**: Hiperparameter that controls how much is going to be penalize. Higher the value, simpler the models. To select the best value, a tuning process must be done. **WARNING** in Lasso, if we pick a large value we can deleate a lot of features because their coefficients will go to 0.\n\n<br>\n\n### a. Ridge Regression (L2)\n\nWhen to use it:\n* High correlation among features.\n* Number of features > number of observations (overdimensional models)\n* When aiming to reduce overfitting while maintaining multicollinearity.\n\nThis model penalizes as follows:\n\n$$\nRSS + \\alpha ·  \\sum_{j=1}^{p} (\\hat{\\beta} _j )^2\n$$\n\nWhere:\n\n* j: Represent a specific feature.\n* p: Denotes total number of features.\n* $\\beta$: Denotes the coeficient of the feature.\n* $\\alpha$: Represent the hiperparameter.\n\n<br>\n\n### b. Lasso Regression (L1)\n\nThis model penalizes as follow:\n\n$$\n\\frac{1}{2n}· RSS + \\alpha ·  \\sum_{j=1}^{p} |\\hat{\\beta}_j|\n$$\n\nWhere:\n\n* j: Denotes a specific feature.\n* p: Represents the total number of features.\n* $\\beta$: Represents the coeficient of the feature.\n* $\\alpha$: Denotes the hiperparameter.\n* n: Represents the number of obervations.\n\n<br>\n\n### c. Elastic Net\n\nCombines the penalties of both Lasso (L1) and Ridge (L2) regression. It aims to overcome the limitations of each method individually by incorporating their strengths.\n\nIn Elastic Net, the loss function is augmented with two penalty terms: one that is proportional to the absolute values of the coefficients (L1 penalty), and another that is proportional to the square of the coefficients (L2 penalty). The objective of Elastic Net is to find a balance between feature selection (like Lasso) and parameter shrinkage (like Ridge).\n\nElastic Net is particularly useful when dealing with datasets where there are multiple correlated features, as it can select groups of correlated features together, unlike Lasso which tends to arbitrarily select only one feature from a group.\n\nThe Elastic Net hyperparameters include $\\alpha$, controlling the overall strength of regularization, and the mixing parameter $\\rho$, determining the balance between L1 and L2 penalties.\n\n  <br>\n  <br>\n  <br>\n        \nWe are going to investigate the regularization options in Python starting with **Ridge Regression**.\nFirst of all, we select different values for the hiperparameter $\\alpha$.","metadata":{}},{"cell_type":"code","source":"# Create the model with different values for alpha\nridge_regression = RidgeCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.400955Z","iopub.status.idle":"2024-05-10T14:25:29.401550Z","shell.execute_reply.started":"2024-05-10T14:25:29.401268Z","shell.execute_reply":"2024-05-10T14:25:29.401292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adjust the model:","metadata":{}},{"cell_type":"code","source":"ridge_model = ridge_regression.fit(df_train_features.sort_index(), df_train_target.sort_index())","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.403574Z","iopub.status.idle":"2024-05-10T14:25:29.404657Z","shell.execute_reply.started":"2024-05-10T14:25:29.404429Z","shell.execute_reply":"2024-05-10T14:25:29.404450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the coefficients and the best value of $\\alpha$:","metadata":{}},{"cell_type":"code","source":"print(\"Coefficients :\", ridge_model.alpha_)\nprint(\"Intercept :\", ridge_model.intercept_ )\n\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, ridge_model.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.406031Z","iopub.status.idle":"2024-05-10T14:25:29.406522Z","shell.execute_reply.started":"2024-05-10T14:25:29.406329Z","shell.execute_reply":"2024-05-10T14:25:29.406346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 4.573 + 0.009 · x_1 -1.037 · x_2 - 0.153 · x_3 + 0.0011 · x_4 + - 1.427 · x_5 + 0.006 · x_6 - 0.003 · x_7 - 0.093 · x_8 - 0.586 · x_9 + 0.861 · x_{10} + 0.309 · x_{11} \n$$\n\n\nAs we see, the polynomial coefficient is negative. It means that when we have higher levels of sulphates, the level of quality (target) decrees. As is a polynomial feature, it means that the decrising on quality is faster.\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features.sort_index().iloc[0:1]) ","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.407945Z","iopub.status.idle":"2024-05-10T14:25:29.408377Z","shell.execute_reply.started":"2024-05-10T14:25:29.408184Z","shell.execute_reply":"2024-05-10T14:25:29.408204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the equation:\n\n$$\n\\hat{y} = 4.573 + 0.009 · 7.8 - 1.037 · 0.88 - 0.153 · 0 + 0.001 · 2.6 - 1.427 · 0.098 + 0.006 · 25 - 0.003 · 67 - 0.093 · 0.997 - 0.586 · 3.2 + 0.861 · 0.68 + 0.309 · 9.8 \\approx 5.188\n$$\n\nIf we compute in python:","metadata":{}},{"cell_type":"code","source":"ridge_model_y = pd.DataFrame(ridge_model.predict(df_train_features), \n                      index=df_train_features.index, \n                      columns=['Ridge Model'])\n\nprint(\"Quality predicted value for the frst case:\", ridge_model_y.sort_index().head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.410515Z","iopub.status.idle":"2024-05-10T14:25:29.411105Z","shell.execute_reply.started":"2024-05-10T14:25:29.410770Z","shell.execute_reply":"2024-05-10T14:25:29.410801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lasso Regression\n\nCreate the Lasso Regression Model and select different values for the hiperparameter $\\alpha$","metadata":{}},{"cell_type":"code","source":"lasso_regression = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.413851Z","iopub.status.idle":"2024-05-10T14:25:29.414361Z","shell.execute_reply.started":"2024-05-10T14:25:29.414095Z","shell.execute_reply":"2024-05-10T14:25:29.414116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adjust the model:","metadata":{}},{"cell_type":"code","source":"lasso_model = lasso_regression.fit(df_train_features, df_train_target)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.416092Z","iopub.status.idle":"2024-05-10T14:25:29.416537Z","shell.execute_reply.started":"2024-05-10T14:25:29.416345Z","shell.execute_reply":"2024-05-10T14:25:29.416363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the coefficients and the best value of $\\alpha$:","metadata":{}},{"cell_type":"code","source":"print(\"Coefficients :\", lasso_model.alpha_)\nprint(\"Intercept :\", lasso_model.intercept_ )\n\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, lasso_model.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.417693Z","iopub.status.idle":"2024-05-10T14:25:29.418092Z","shell.execute_reply.started":"2024-05-10T14:25:29.417901Z","shell.execute_reply":"2024-05-10T14:25:29.417918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 3.981 + 0.012 · x_1 -1.012 · x_2 - 0.074 · x_3 + 0.004 · x_4 + - 0.803 · x_5 + 0.006 · x_6 - 0.003 · x_7 - 0 · x_8 - 0.457 · x_9 + 0.768 · x_{10} + 0.311 · x_{11} \n$$\n\n\nAs we see, tthe coefficient for *density* drops to 0\n\nIf we replace the features for our first case we obtain this:","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features.sort_index().iloc[0:1]) ","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.419567Z","iopub.status.idle":"2024-05-10T14:25:29.420025Z","shell.execute_reply.started":"2024-05-10T14:25:29.419827Z","shell.execute_reply":"2024-05-10T14:25:29.419846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the equation:\n\n$$\n\n\\hat{y} = 3.981 + 0.012 · 7.8 -1.012 · 0.88 - 0.074 · 0 + 0.004 · 2.6 + - 0.803 · 0.098 + 0.006 · 25 - 0.003 · 67 - 0 · 0.997 - 0.457 · 3.2 + 0.768 · 0.68 + 0.311 · 9.8 \\approx 5.172\n\n$$\n\nIf we compute in python:","metadata":{}},{"cell_type":"code","source":"lasso_model_y = pd.DataFrame(lasso_model.predict(df_train_features), \n                      index=df_train_features.index, \n                      columns=['Lasso Model'])\n\nprint(\"Quality predicted value for the first case:\", lasso_model_y.sort_index().head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.423021Z","iopub.status.idle":"2024-05-10T14:25:29.423917Z","shell.execute_reply.started":"2024-05-10T14:25:29.423671Z","shell.execute_reply":"2024-05-10T14:25:29.423694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Quantiy the fit\n\n### A. Coefficient of determination $R^2$\n\nDefinition: \"Proportion of target varianze that could be predict using the targeys\".\n\nThis coefficient is used to compare the efficiency of different models and determine which of them fix better the observate data.\n\nThe formula is as follow:\n\n$$\nR^2 = 1 - \\frac{SS_{res}}{SS_{total}}\n$$\n\n* Residual Sum Square ($SS_{res}$): \n\n$$\nSS_{res} = \\sum(y_i - \\hat{y_i})^2\n$$\n\n* Total Sum Square ($SS_{total}$): \n\n$$\nSS_{total} = \\sum(y_i - \\bar{y_i})^2\n$$\n\nThe nomenclature used in this is formulas:\n\n* $y_i$: Real values of the target.\n* $\\hat{y_i}$: Target values predicted by the model.\n* $\\bar{y_i}$: Mean of the real values ($y_i$).\n\nInterpretation:\n\n* $R^2$ = 1. The model is perfect. The features explain all the variability of the target. The residues are minimus.\n* $R^2$ = 0. The model doesn´t explain the variability in the data. Is not more useful than use the mean of the data as a predictor of each observation.\n* $R^2$ < 0. The model made worse than using simply the mean.\n\nCharacteristics:\n\n* Although it is odd, the model could be worse than use only the mean.\n\n* When the prediction residuals have zero mean, the score is identical to the Explained Variance score.\n\nWe are going to explore the models that we generate in previous steps:","metadata":{}},{"cell_type":"code","source":"# Sort by index\ndf_train_target = df_train_target.sort_index()\nmodel1_y = model1_y.sort_index()\nmodel2_y = model2_y.sort_index()\nmodel3_y = model3_y.sort_index()\nmodel4_y = model4_y.sort_index()\nridge_model_y = ridge_model_y.sort_index()\nlasso_model_y = lasso_model_y.sort_index()\n\n# Ensure all preidctions and real observations has the same index:\nassert df_train_target.index.equals(model1_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model2_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model3_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(model4_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(ridge_model_y.index), \"Indices do not match\"\nassert df_train_target.index.equals(lasso_model_y.index), \"Indices do not match\"\nprint(\"Indices match successfully!\")\n\npredictions = pd.DataFrame({\n    'Actual Target': np.squeeze(df_train_target), # Real values\n    'Simple Model': np.squeeze(model1_y),\n    'Multiple Model': np.squeeze(model2_y),\n    'Interaction Model': np.squeeze(model3_y),\n    'Polynomial Model': np.squeeze(model4_y),\n    'Ridge Model': np.squeeze(ridge_model_y),\n    'Lasso Model': np.squeeze(lasso_model_y),\n})\n\nprint(predictions.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.426634Z","iopub.status.idle":"2024-05-10T14:25:29.427324Z","shell.execute_reply.started":"2024-05-10T14:25:29.426892Z","shell.execute_reply":"2024-05-10T14:25:29.426909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to calculate the $R^2$ coefficient:","metadata":{}},{"cell_type":"code","source":"r2_scores = {}\nr2_scores['Simple Model'] = r2_score(predictions['Actual Target'], predictions['Simple Model'])\nr2_scores['Multiple Model'] = r2_score(predictions['Actual Target'], predictions['Multiple Model'])\nr2_scores['Interaction Model'] = r2_score(predictions['Actual Target'], predictions['Interaction Model'])\nr2_scores['Polynomial Model'] = r2_score(predictions['Actual Target'], predictions['Polynomial Model'])\nr2_scores['Ridge Model'] = r2_score(predictions['Actual Target'], predictions['Ridge Model'])\nr2_scores['Lasso Model'] = r2_score(predictions['Actual Target'], predictions['Lasso Model'])\n\n# Print the R2 scores\nfor model, score in r2_scores.items():\n    print(f\"R2 score for {model}: {score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.428682Z","iopub.status.idle":"2024-05-10T14:25:29.429088Z","shell.execute_reply.started":"2024-05-10T14:25:29.428904Z","shell.execute_reply":"2024-05-10T14:25:29.428920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model that includes all the features without transformation is the model that works better. On the other side, model where were transform one feature (interaction and polynomial) are the ones that shows the wors behaviour.","metadata":{}},{"cell_type":"markdown","source":"### B. Adjusted coefficient of determination $R^2$\n\n**Why to use it?**: When we add more predictiors to the model, $R^2$ always increase (or at least doesn´t decrease). The problem is that adjusted is not as inerpretable than $R^2$.\n\nFormula looks like follows:\n\n$$\nR^2 = 1 - \\left( \\frac{SS_{\\text{res}}}{SS_{\\text{total}}} \\right) \\cdot \\left( \\frac{N-1}{N-K-1} \\right)\n$$\n\nWhere:\n\n* $SS_{\\text{res}}$ and $SS_{\\text{total}}$ are the same as in $R^2$.\n* **N**: Number of observations in the data.\n* **K**: Number of features used in the model (excluding the intercept)\n\nThe second part of the formula is the part that adjust for Degrees of Freedom.","metadata":{}},{"cell_type":"markdown","source":"# 5. Hypothesis tests\n\nTwo aproximations:\n\n* A. See if the model proposed is better than the null model.\n* B. Test if a particular coefficient is statistical different from 0\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### A. Compare with the null model\n\nThe null model (baseline model) is that one that no use any feature to predict the target.\n\nThe null hypothesis is that there are no relation between the features and the target.\n\n**Null model**: Consist in that model that predict using only the mean or the median\n\n$$\n\\hat{y_i} = \\bar{y_i}:\n$$\n\nWhere:\n\n* $\\hat{y_i}$: Prediction.\n* $\\bar{y_i}$: Mean of the target values in the training set.\n\nWe can follow three strategies:\n\n#### a. Difference in $R^2$\n\nCompare the values of the coefficient of determination between the null model and an alternative model. Following this we are going to know what model works better, but we can ensure if one works significatilly better than the other. \n\n#### b. F-test for nested models (Extra Sum of Squares Test)\n\nThis approach is used when the alternative modell is equal to the null model plus some additional predictors.\n\nWe calculate if the explained variance in the full modell is significantly greater than in the null model relative to the increase in degrees of fredoon (number of predictors).\n\nFormula:\n\n$$\nF = \\frac{\\frac{(R^2_{\\text{full}} - R^2_{\\text{reduced}})}{p}}{\\frac{(1 - R^2_{\\text{full}})}{(n - p - 1)}} \n$$\n\n\nWhere:\n* $R^2_{\\text{full}}$ is the R-squared of the full model (with the additional predictors).\n* $R^2_{\\text{reduced}}$ is the R-squared of the reduced model (null model).\n* $p$ is the number of extra predictors added to the reduced model to get the full model.\n* $n$ is the total number of observations.\n\n### c. Information Criteria (AIC\\BIC)\n\n* **AIC**: Akaike Information Criterion. Focus on selecting the model that most closely approximates the truth. Disadvantage: It can favor mode complex model (risk of overfitting). Use when all model to compare have the same complexity or when the models are nested.\n* **BIC**: Bayesian Information Criterion. Strong penalty for complexity, often favoring simpler model especially as sample size grows. Preferable when dealing with very large datasets.\n\n**Characteristics**:\n\n* Meassure the complexity of the model penalizing unnecessary complexity (too much predictors). \n* Lowe values of AIC and BIC means a better model.\n* A significant drop in AIC and BIC values when move from null model to the alternative model can suggest that the increase in the model complexity is justified.\n* Interesting when you are interested in model selection or balancing models.\n\n**Interpretation**: There is no way to test if the improvements is significative. However, there is a rule of thumb. We can apply for both, AIC and BIC:\n\n* Diferences 2: No siginificative.\n* Differences between 2 and 6: Some evidence against the model with the higher value of the criterion. This means the model with the lower AIC\\BIC is better.\n* Differences between 6 and 10: Strong evidence that the model with the lower criterios is better.\n* Difference > 10. Very strong evidence.\n\n### Now we are going to implement these three approachs in Python:\n\n#### a.  Difference in $R^2$\nWe are going to create the null value and compare with the other models we have, then we use the F-test approach and finally the Information Criteria approach.","metadata":{}},{"cell_type":"code","source":"# Empty model: Always predict the mean\nmean_regressor = DummyRegressor(strategy='mean')\nmean_regressor.fit(df_train_features.sort_index(), df_train_target.sort_index())\ntarget_pred_mean = mean_regressor.predict(df_train_target.sort_index())\nprint(target_pred_mean)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.431444Z","iopub.status.idle":"2024-05-10T14:25:29.432040Z","shell.execute_reply.started":"2024-05-10T14:25:29.431740Z","shell.execute_reply":"2024-05-10T14:25:29.431764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we compute $R^2$ for our nul model:","metadata":{}},{"cell_type":"code","source":"r2_null_model = r2_score(df_train_target, target_pred_mean)\nprint(\"R² del Modelo Vacío:\", r2_null_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.433243Z","iopub.status.idle":"2024-05-10T14:25:29.433806Z","shell.execute_reply.started":"2024-05-10T14:25:29.433526Z","shell.execute_reply":"2024-05-10T14:25:29.433549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now compare the $R^2$ of each model:","metadata":{}},{"cell_type":"code","source":"# Predictions as df\ndf_r2_scores = pd.DataFrame(list(r2_scores.items()), columns=['Model', 'R-squared'])\n\n# Add the new column\nnull_model_row = pd.DataFrame({'Model': ['Null Model'], 'R-squared': [r2_null_model]})\ndf_r2_scores = pd.concat([df_r2_scores, null_model_row], ignore_index=True)\n\nprint(df_r2_scores)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.435234Z","iopub.status.idle":"2024-05-10T14:25:29.435811Z","shell.execute_reply.started":"2024-05-10T14:25:29.435531Z","shell.execute_reply":"2024-05-10T14:25:29.435554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the all the models predict better than the null model.\n\n#### b. F-test for nested models (Extra Sum of Squares Test)\n\nIn stat model we need to include a constant if we want an intercept in the model.","metadata":{}},{"cell_type":"code","source":"# Creare a df for the null model. As we don´t have predictors, only a constant is present.\nnull_features   = sm.add_constant(pd.DataFrame(index=df_train_features.index)).sort_index()\nfull_features   = sm.add_constant(df_train_features).sort_index()\n\n#reduce_model = sm.add_constant(X_reduced)\nprint(\"Null features:\\n\", null_features.tail(5), \"\\nMultiple Regression features:\\n\", full_features.tail(5))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.437280Z","iopub.status.idle":"2024-05-10T14:25:29.437840Z","shell.execute_reply.started":"2024-05-10T14:25:29.437563Z","shell.execute_reply":"2024-05-10T14:25:29.437586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use OLS to fit each model:","metadata":{}},{"cell_type":"code","source":"# Fit the models using OLS\nmodel_null = sm.OLS(df_train_target.sort_index(), null_features.sort_index()).fit()\nmodel_full = sm.OLS(df_train_target.sort_index(), full_features.sort_index()).fit()\n\n# Summary of the models\nprint(\"Null Model Summary:\")\nprint(model_null.summary())\nprint(\"\\nFull Model Summary:\")\nprint(model_full.summary())","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.439455Z","iopub.status.idle":"2024-05-10T14:25:29.440006Z","shell.execute_reply.started":"2024-05-10T14:25:29.439734Z","shell.execute_reply":"2024-05-10T14:25:29.439757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we compare if full model is nested within null model:","metadata":{}},{"cell_type":"code","source":"f_test = model_full.compare_f_test(model_null)\nprint(\"\\nF-test result:\", f_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.441375Z","iopub.status.idle":"2024-05-10T14:25:29.441935Z","shell.execute_reply.started":"2024-05-10T14:25:29.441652Z","shell.execute_reply":"2024-05-10T14:25:29.441674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results**:\n\n* **F statistic value**: 65.713. This is the calculated F-statistic for the test. It represents the ratio of the model fit improvement per added predictor to the error of the larger model (including all predictors). A higher F-statistic indicates that the additional predictors in the full model provide a significant improvement in fit over the reduced model.\n* **P-value**: 5.379-116 = <0.001. Probability under the null hypothesis of observing the F-statistic, or one more extreme, given that the null hypothesis is true. Here, the null hypothesis typically states that the additional predictors in the full model do not improve the fit of the model meaningfully compared to the reduced model. A very small p-value, as in this case (which is virtually zero), strongly suggests rejecting the null hypothesis. This means that the additional predictors do significantly improve the model.\n* **Degrees of Freedom**: 11.0. This number represents the degrees of freedom associated with the numerator of the F-statistic, which is generally equal to the number of additional parameters added to the reduced model to get the full model. Here, 11 extra parameters were added to the reduced model to form the full model.\n\n**Overall conclusion**: The features in the full model improve the prediction VS the null model.","metadata":{}},{"cell_type":"markdown","source":"#### c. Information criteria (AIC/BIC)\n\nSame steps as in b to obtain model_null and model_full. As we have these step done yet, we pass to see the AIC and BIC criterials in each model:","metadata":{}},{"cell_type":"code","source":"print(\"Null model AIC:\", round(model_null.aic, 3), \"Full model AIC:\", round(model_full.aic, 3))\nprint(\"Null model BIC:\", round(model_null.bic, 3), \"Full model BIC:\", round(model_full.bic, 3))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.443322Z","iopub.status.idle":"2024-05-10T14:25:29.443702Z","shell.execute_reply.started":"2024-05-10T14:25:29.443526Z","shell.execute_reply":"2024-05-10T14:25:29.443542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We compute Deltas (differences) to interpretate the results:","metadata":{}},{"cell_type":"code","source":"# Diferences:\ndelta_aic = model_null.aic - model_full.aic\ndelta_bic = model_null.bic - model_full.bic\n\nprint(\"Delta AIC:\", round(delta_aic), \"\\nDelta BIC:\", round(delta_bic))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.448068Z","iopub.status.idle":"2024-05-10T14:25:29.448574Z","shell.execute_reply.started":"2024-05-10T14:25:29.448351Z","shell.execute_reply":"2024-05-10T14:25:29.448371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the differences are large so we can conclude the full model is better making predictions than the null model.","metadata":{}},{"cell_type":"markdown","source":"### B. Test if a particular coefficient is statistical different from 0\n\nThis approach let as know each predictor in the model separately. We can test if a particular predictor have a statistically significant relationship with the target. If the coefficient is statistically different from 0, then wecan conclude there is a relationship.\n\nScikit-learn package has no a direct way to compute it but we can fit the model with Scikit-Learn and then use the statsmodel from package stats to explore the statistical summary.\n\nWe are going to use the model with all features that we compute in the Multiple linear regression section.","metadata":{}},{"cell_type":"code","source":"print(model2.intercept_)\nprint(model2.coef_)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.450541Z","iopub.status.idle":"2024-05-10T14:25:29.450983Z","shell.execute_reply.started":"2024-05-10T14:25:29.450781Z","shell.execute_reply":"2024-05-10T14:25:29.450799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we obtain the statistical summary using statsmodels. This summary includes the standard errors, t-statistics and p-values of the coefficients. We had do this in the previous section so:","metadata":{}},{"cell_type":"code","source":"print(model_full.summary())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-10T14:25:29.452352Z","iopub.status.idle":"2024-05-10T14:25:29.452806Z","shell.execute_reply.started":"2024-05-10T14:25:29.452602Z","shell.execute_reply":"2024-05-10T14:25:29.452620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table above we have a list with all the features informations as well as the intercept.\n\n**Coefficients statistical significative** at 0.05 level:\n\n* Volatile acidity, chlorides,, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\n# 6. Standardised coefficients\n\nIt is usual that our variables (target) have different scales. The unirs of measurement influence on the refression coefficients.\n\n\n**Standardised coefficients**: Coefficients obtained when convert all the variables to standard scores ($z-score$) **BEFORE** run the regression. When we standarized we make the variable have mean 0 and standard deviation 1. \nWhen we standarized we make the variable have mean 0 and standard deviation 1. \n\n**Reasons to use**:\n\n* It is specially important when we want to compare the strenght of different predictors (features).It is specially important when we want to compare the strenght of different predictors (features). An absolute value of $\\beta$ means a stronger relationship with the target.\n* Coefficients of standardized variables represent the change in the dependent variable for a one standard deviation change in the predictor, making it easier to compare the effects of different variables.Coefficients of standardized variables represent the change in the dependent variable for a one standard deviation change in the predictor, making it easier to compare the effects of different variables.\n* Moreover, when we use regularization, standarization is crital because it penalize the coefficients base on their size. \n* Acelerate convergence in algorithms that use gradient descent (like logistic regression or when using regularitation).\n\nWe will see how to do this in python step by step:\n\nStandardize the VariablesStandardize the Variables:","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\ndf_train_features_stand = scaler.fit_transform(df_train_features)\nprint(df_train_features_stand)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.454076Z","iopub.status.idle":"2024-05-10T14:25:29.454543Z","shell.execute_reply.started":"2024-05-10T14:25:29.454347Z","shell.execute_reply":"2024-05-10T14:25:29.454367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use StandarScaler() it is necessary transform our data into an array. We need to recover the index and the columns names before made the regression","metadata":{}},{"cell_type":"code","source":"df_train_features_stand = pd.DataFrame(df_train_features_stand, \n                                       index=df_train_features.index, \n                                       columns=df_train_features.columns).sort_index()\n\n# Print the standardized features with the DataFrame structure\nprint(df_train_features_stand.head(11))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.455825Z","iopub.status.idle":"2024-05-10T14:25:29.456260Z","shell.execute_reply.started":"2024-05-10T14:25:29.456043Z","shell.execute_reply":"2024-05-10T14:25:29.456059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create and fit the model:","metadata":{}},{"cell_type":"code","source":"# Create MLRM and fit it\nregression_stand = LinearRegression()\n\n# Fit the model\nmodel_stand = regression_stand.fit(df_train_features_stand.sort_index(),\n                                   df_train_target, \n                                   ) # Features first then target\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.458275Z","iopub.status.idle":"2024-05-10T14:25:29.458709Z","shell.execute_reply.started":"2024-05-10T14:25:29.458507Z","shell.execute_reply":"2024-05-10T14:25:29.458525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can print the intercept and the coefficients:","metadata":{}},{"cell_type":"code","source":"# Show the equation values:\nprint(\"Intercept:\", model_stand.intercept_)\n\n# New way to obtain the coefficients with names and round them to three decimals:\nfeature_names = df_train_features.columns\ncoefficient_dict = dict(zip(feature_names, model_stand.coef_))\nprint(\"Coefficients :\")\nfor feature, coef in coefficient_dict.items():\n    print(f\"{feature}: {coef:.3f}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.460321Z","iopub.status.idle":"2024-05-10T14:25:29.460736Z","shell.execute_reply.started":"2024-05-10T14:25:29.460541Z","shell.execute_reply":"2024-05-10T14:25:29.460557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have all standarized, we are going to interpretate the intercept and coefficients:\n\n* **Intercept**. (5.625). Expected value when all the predictors (features) are 0. This is the same to say that is the value predicted when all the coefficients take their means. If we see above, this is the predicted value in the null model.\n\n* **Fixed Acidity**. (0.035): A one standard deviation increase in fixed acidity is associated with a 0.035 standard deviation increase in the response variable.\n* **Volatile Acidity**. (-0.185): A one standard deviation increase in volatile acidity leads to a 0.185 standard deviation decrease in the response variable, suggesting a negative relationship.\n* **Citric Acid**. (-0.029):A one standard deviation increase in citric acid results in a 0.029 standard deviation decrease in the response variable.\n* **Residual Sugar**. (0.016): A one standard deviation increase in residual sugar is associated with a 0.016 standard deviation increase in the response variable.\n* **Chlorides**. (-0.065): A one standard deviation increase in chlorides results in a 0.065 standard deviation decrease in the response variable.\n* **Free Sulfur Dioxide**. (0.056): A one standard deviation increase in free sulfur dioxide is linked to a 0.056 standard deviation increase in the response variable.\n* **Total Sulfur Dioxide**. (-0.112): A one standard deviation increase in total sulfur dioxide leads to a 0.112 standard deviation decrease in the response variable.\n* **Density**. (-0.023): A one standard deviation increase in density results in a 0.023 standard deviation decrease in the response variable.\n* **pH**. (-0.082): A one standard deviation increase in pH is associated with a 0.082 standard deviation decrease in the response variable.\n* **Sulphates**. (0.145): A one standard deviation increase in sulphates leads to a 0.145 standard deviation increase in the response variable.\n* **Alcohol**. (0.317): A one standard deviation increase in alcohol is associated with a 0.317 standard deviation increase in the response variable, making it the most impactful predictor among those listed.\n","metadata":{}},{"cell_type":"markdown","source":"Now build the equation of the model:\n\n$$\n\\hat{y} = 5.625 + 0.035 · x_1 -0.185 · x_2 - 0.029 · x_3 + 0.016 · x_4  - 0.065 · x_5 + 0.056 · x_6 - 0.112 · x_7 - 0.023 · x_8 - 0.082 · x_9 + 0.145 · x_{10} + 0.317 · x_{11} \n$$\n\n\nAs we see,  coefficient take the same signe (positive/negative) that in other models\n\nIf we replace the features for our first case we obtain this (use the standarized features):","metadata":{}},{"cell_type":"code","source":"print(\"Value of all features for our first case :\", df_train_features_stand.sort_index().iloc[0:1]) ","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.462541Z","iopub.status.idle":"2024-05-10T14:25:29.462952Z","shell.execute_reply.started":"2024-05-10T14:25:29.462768Z","shell.execute_reply":"2024-05-10T14:25:29.462785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The equation for our first case is like:\n\n$$\n\\hat{y} = 5.625 + 0.035 · (-0.302) -0.185 ·  (- 1.922) -0.029 · (-1.381) + 0.016 · 0.058  - 0.065 · 0.259 + 0.056 · 0.896 - 0.112 · 0.613 - 0.023 · 0.025 - 0.082 · (-0.728) + 0.145 · 0.155 + 0.317 · (-0.578) \\approx  5.883\n\n\n$$\n","metadata":{}},{"cell_type":"code","source":"model_stand_y = pd.DataFrame(model_stand.predict(df_train_features_stand), \n                      index=df_train_features_stand.index, \n                      columns=['Standarized model'])\n\nprint(\"Quality predicted value for the first case:\", model_stand_y.sort_index().head(1))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T14:25:29.464761Z","iopub.status.idle":"2024-05-10T14:25:29.465218Z","shell.execute_reply.started":"2024-05-10T14:25:29.464991Z","shell.execute_reply":"2024-05-10T14:25:29.465009Z"},"trusted":true},"execution_count":null,"outputs":[]}]}